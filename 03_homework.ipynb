{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1c678e33-7efc-47da-bd84-890daf4b5beb",
   "metadata": {},
   "source": [
    "# Задание 1\n",
    "\n",
    "Имплементируйте алгоритм Леска (описание есть в семинаре) и оцените качество его работы на датасете `data/corpus_wsd_50k.txt`\n",
    "\n",
    "В качестве метрики близости вы должны попробовать два подхода:\n",
    "\n",
    "1) Jaccard score на множествах слов (определений и контекста)\n",
    "2) Cosine distance на эмбедингах sentence_transformers\n",
    "\n",
    "В качестве метрики используйте accuracy (% правильных ответов). Предсказывайте только многозначные слова в датасете\n",
    "\n",
    "Контекст вы можете определить самостоятельно (окно вокруг целевого слова или все предложение). Также можете поэкспериментировать с предобработкой для обоих методов."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "153a4330-4140-4507-a743-18264a8ae784",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_wsd = []\n",
    "corpus = open('data/corpus_wsd_50k.txt').read().split('\\n\\n')\n",
    "for sent in corpus:\n",
    "    corpus_wsd.append([s.split('\\t') for s in sent.split('\\n')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "207bd163",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import wordnet as wn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5814a100-9ba8-4787-b0ac-73fa657a3fa4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['', 'how', 'How'],\n",
       " ['long%3:00:02::', 'long', 'long'],\n",
       " ['', 'have', 'has'],\n",
       " ['', 'it', 'it'],\n",
       " ['be%2:42:03::', 'be', 'been'],\n",
       " ['', 'since', 'since'],\n",
       " ['', 'you', 'you'],\n",
       " ['review%2:31:00::', 'review', 'reviewed'],\n",
       " ['', 'the', 'the'],\n",
       " ['objective%1:09:00::', 'objective', 'objectives'],\n",
       " ['', 'of', 'of'],\n",
       " ['', 'you', 'your'],\n",
       " ['benefit%1:21:00::', 'benefit', 'benefit'],\n",
       " ['', 'and', 'and'],\n",
       " ['service%1:04:07::', 'service', 'service'],\n",
       " ['program%1:09:01::', 'program', 'program'],\n",
       " ['', '?', '?']]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus_wsd[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "be9304db",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Создаем список слов для сравнения\n",
    "disamb = list()\n",
    "for sentence in corpus_wsd:\n",
    "    for word in sentence:\n",
    "        if word[0] != '':\n",
    "            disamb.append(word[1])\n",
    "\n",
    "#disamb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "22080275",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Создаем список ключей к определениям в wn\n",
    "keys = list()\n",
    "for sentence in corpus_wsd:\n",
    "    for word in sentence:\n",
    "        if word[0] != '':\n",
    "            keys.append(word[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Создаем список контекстов для сравнения\n",
    "contexts = list()\n",
    "for sentence in corpus_wsd:\n",
    "    for word in sentence:\n",
    "        if word[0] != '':\n",
    "            context = list()\n",
    "            x = word[2]\n",
    "            for word in sentence:\n",
    "                if word[2] != x:\n",
    "                    context.append(word[2])\n",
    "                else:\n",
    "                    context.append('_')\n",
    "\n",
    "            context = ' '.join(context)\n",
    "            contexts.append(context)\n",
    "\n",
    "#contexts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3922cb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = 'employee'\n",
    "for synset in wn.synsets(x):\n",
    "    print(word + ' - ' + synset.definition())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "7f992092",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['long', 'be', 'review', 'objective', 'benefit', 'service', 'program', 'permit', 'become', 'giveaway', 'program', 'rather', 'have', 'goal', 'improved', 'employee', 'morale', 'consequently', 'increased', 'productivity']\n"
     ]
    }
   ],
   "source": [
    "print(disamb[:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34f83b1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Достать список определений \n",
    "#для каждого слова из списка слов для сравнения\n",
    "\n",
    "all_definitions = list()\n",
    "for word in disamb:\n",
    "    definitions = list()\n",
    "    for synset in wn.synsets(word):\n",
    "        definitions.append(synset.definition())\n",
    "    all_definitions.append(definitions)\n",
    "\n",
    "#all_definitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95dd4d90",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Получаем список наиболее подходящих определений по Jaccard\n",
    "\n",
    "jaccard_defs = list()\n",
    "\n",
    "for definitions in all_definitions:\n",
    "    count = 0\n",
    "    max = float(0)\n",
    "    defs = list()\n",
    "    for definition in definitions:\n",
    "        intersection = (set(definition.split()) & set(contexts[count].split()))\n",
    "        union = (set(definition.split()) | set(contexts[count].split()))\n",
    "        jaccard = len(intersection) / len(union)\n",
    "        if jaccard > max:\n",
    "            max = jaccard\n",
    "            defs.append(definition)\n",
    "    if defs != []:\n",
    "        jaccard_defs.append(defs[-1])\n",
    "    else:\n",
    "        jaccard_defs.append(defs)\n",
    "    count += 1\n",
    "\n",
    "jaccard_defs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "eeed0b5b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'planning prudently for the future'"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "jaccard_defs[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "fc6f61bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Сравниваем полученные определения с данными в корпусе\n",
    "counter = 0\n",
    "volume = len(disamb)\n",
    "count = 0\n",
    "for definition in jaccard_defs:\n",
    "    lemma = wn.lemma_from_key(keys[count]).synset().definition()\n",
    "    if definition == lemma:\n",
    "        counter += 1\n",
    "\n",
    "    count += 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "9842f337",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "239913"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(jaccard_defs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "889cb4fe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "239913"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(contexts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "fe324c3f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "239913"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(keys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "51edc402",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "239913"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "volume"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "85fec69a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'23%'"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Считаем точность\n",
    "accuracy = format(counter/volume, '.0%')\n",
    "accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e360626",
   "metadata": {},
   "source": [
    "Теперь с косинусной близостью"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "55e87a9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_distances, cosine_similarity\n",
    "from sentence_transformers import SentenceTransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3b12cab0",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SentenceTransformer('sentence-transformers/all-mpnet-base-v2')\n",
    "embed = model.encode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "e46ce199",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.10057071]], dtype=float32)"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "definition00 = 'a system of projects or services intended to meet a public need'\n",
    "definition02 = '(computer science) a sequence of instructions that a computer can interpret and execute'\n",
    "context00 = 'i wrote a computer _ that predicts the sense of a word'\n",
    "cossim00 = cosine_similarity(embed(definition00).reshape(1, -1), embed(context00).reshape(1, -1))\n",
    "cossim02 = cosine_similarity(embed(definition02).reshape(1, -1), embed(context00).reshape(1, -1))\n",
    "cossim00\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "bf3cbe49",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.3182025]], dtype=float32)"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cossim02"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "863c6032",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Smaller\n"
     ]
    }
   ],
   "source": [
    "if cossim00 > cossim02:\n",
    "    print('Larger')\n",
    "elif cossim02 > cossim00:\n",
    "    print ('Smaller')\n",
    "else:\n",
    "    print('EQ')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b68e4340",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Берем определение по наибольшей косинусной близости\n",
    "cos_defs = list()\n",
    "\n",
    "for definitions in all_definitions:\n",
    "    count2 = 0\n",
    "    max2 = float(0)\n",
    "    defs2 = list()\n",
    "    for definition in definitions:\n",
    "        cossim = cosine_similarity(embed(definition).reshape(1, -1), embed(contexts[count2]).reshape(1, -1))\n",
    "        \n",
    "        if cossim > max2:\n",
    "            max2 = cossim\n",
    "            defs2.append(definition)\n",
    "    if defs2 != []:\n",
    "        cos_defs.append(defs2[-1])\n",
    "    else:\n",
    "        cos_defs.append(defs2)\n",
    "    count2 += 1\n",
    "\n",
    "cos_defs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b7174fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "counter2 = 0\n",
    "volume2 = len(disamb)\n",
    "count3 = 0\n",
    "for definition in cos_defs:\n",
    "    lemma = wn.lemma_from_key(keys[count3]).synset().definition()\n",
    "    if definition == lemma:\n",
    "        counter2 += 1\n",
    "\n",
    "    count3 += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16d69397",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Считаем точность тут\n",
    "accuracy2 = format(counter2/volume, '.0%')\n",
    "accuracy2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39efde9a-af0b-4c94-bfd0-249e7054562f",
   "metadata": {},
   "source": [
    "# Задание 2\n",
    "Попробуйте разные алгоритмы кластеризации на датасете - `https://github.com/nlpub/russe-wsi-kit/blob/initial/data/main/wiki-wiki/train.csv`\n",
    "\n",
    "Используйте код из семинара как основу. Используйте ARI как метрику качества.\n",
    "\n",
    "Попробуйте все 4 алгоритма кластеризации, про которые говорилось на семинаре. Для каждого из алгоритмов попробуйте настраивать гиперпараметры (посмотрите их в документации). Прогоните как минимум 5 экспериментов (не обязательно успешных) с разными параметрами на каждый алгоритме кластеризации и оцените: качество кластеризации, скорость работы, интуитивность параметров.\n",
    "\n",
    "Помимо этого также выберите 1 дополнительный алгоритм кластеризации отсюда - https://scikit-learn.org/stable/modules/clustering.html , опишите своими словами принцип его работы  и проделайте аналогичные эксперименты. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d59bef3e-1af7-4ce2-b43a-dfef282050f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f3f1c240",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('data/train.csv', sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41dcf243",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5aabb025",
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped_df = df.groupby('word')[['word', 'context', 'gold_sense_id']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad590997",
   "metadata": {},
   "outputs": [],
   "source": [
    "for key, _ in grouped_df:\n",
    "    print(grouped_df.get_group(key), \"\\n\\n\")\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "73bdc4ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans, DBSCAN, AffinityPropagation\n",
    "import numpy as np\n",
    "from sklearn.metrics import adjusted_rand_score\n",
    "\n",
    "from IPython.display import Image\n",
    "from IPython.core.display import HTML"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13d3c4fa",
   "metadata": {},
   "source": [
    "Смотрим KMeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f98732f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Dmitry\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1416: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  super()._check_params_vs_input(X, default_n_init=10)\n",
      "c:\\Users\\Dmitry\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1416: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  super()._check_params_vs_input(X, default_n_init=10)\n",
      "c:\\Users\\Dmitry\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1416: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  super()._check_params_vs_input(X, default_n_init=10)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.04802335780315568\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Dmitry\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1416: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  super()._check_params_vs_input(X, default_n_init=10)\n"
     ]
    }
   ],
   "source": [
    "ARI = []\n",
    "\n",
    "for key, _ in grouped_df:\n",
    "    # вытаскиваем контексты\n",
    "    texts = grouped_df.get_group(key)['context'].values\n",
    "\n",
    "    # создаем пустую матрицу для векторов \n",
    "    X = np.zeros((len(texts), 768))\n",
    "\n",
    "    # переводим тексты в векторы и кладем в матрицу\n",
    "    for i, text in enumerate(texts):\n",
    "        X[i] = embed(text)\n",
    "\n",
    "    # выбираем один из алгоритмов\n",
    "    # cluster = AffinityPropagation(damping=0.9)\n",
    "    cluster = KMeans(3)\n",
    "    # cluster = DBSCAN(min_samples=1, eps=0.1)\n",
    "    \n",
    "    cluster.fit(X)\n",
    "    labels = np.array(cluster.labels_)+1 \n",
    "\n",
    "    # расчитываем метрику для отдельного слова\n",
    "    ARI.append(adjusted_rand_score(grouped_df.get_group(key)['gold_sense_id'], labels))\n",
    "    \n",
    "print(np.mean(ARI)) # усредненная метрика"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8c737703",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Dmitry\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1416: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  super()._check_params_vs_input(X, default_n_init=10)\n",
      "c:\\Users\\Dmitry\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1416: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  super()._check_params_vs_input(X, default_n_init=10)\n",
      "c:\\Users\\Dmitry\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1416: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  super()._check_params_vs_input(X, default_n_init=10)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.07730266389650145\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Dmitry\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1416: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  super()._check_params_vs_input(X, default_n_init=10)\n"
     ]
    }
   ],
   "source": [
    "ARI = []\n",
    "\n",
    "for key, _ in grouped_df:\n",
    "    # вытаскиваем контексты\n",
    "    texts = grouped_df.get_group(key)['context'].values\n",
    "\n",
    "    # создаем пустую матрицу для векторов \n",
    "    X = np.zeros((len(texts), 768))\n",
    "\n",
    "    # переводим тексты в векторы и кладем в матрицу\n",
    "    for i, text in enumerate(texts):\n",
    "        X[i] = embed(text)\n",
    "\n",
    "    # выбираем один из алгоритмов\n",
    "    # cluster = AffinityPropagation(damping=0.9)\n",
    "    cluster = KMeans(5)\n",
    "    # cluster = DBSCAN(min_samples=1, eps=0.1)\n",
    "    \n",
    "    cluster.fit(X)\n",
    "    labels = np.array(cluster.labels_)+1 \n",
    "\n",
    "    # расчитываем метрику для отдельного слова\n",
    "    ARI.append(adjusted_rand_score(grouped_df.get_group(key)['gold_sense_id'], labels))\n",
    "    \n",
    "print(np.mean(ARI)) # усредненная метрика"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d90e0f6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Dmitry\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1416: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  super()._check_params_vs_input(X, default_n_init=10)\n",
      "c:\\Users\\Dmitry\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1416: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  super()._check_params_vs_input(X, default_n_init=10)\n",
      "c:\\Users\\Dmitry\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1416: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  super()._check_params_vs_input(X, default_n_init=10)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.05481698793321215\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Dmitry\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1416: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  super()._check_params_vs_input(X, default_n_init=10)\n"
     ]
    }
   ],
   "source": [
    "ARI = []\n",
    "\n",
    "for key, _ in grouped_df:\n",
    "    # вытаскиваем контексты\n",
    "    texts = grouped_df.get_group(key)['context'].values\n",
    "\n",
    "    # создаем пустую матрицу для векторов \n",
    "    X = np.zeros((len(texts), 768))\n",
    "\n",
    "    # переводим тексты в векторы и кладем в матрицу\n",
    "    for i, text in enumerate(texts):\n",
    "        X[i] = embed(text)\n",
    "\n",
    "    # выбираем один из алгоритмов\n",
    "    # cluster = AffinityPropagation(damping=0.9)\n",
    "    cluster = KMeans(10)\n",
    "    # cluster = DBSCAN(min_samples=1, eps=0.1)\n",
    "    \n",
    "    cluster.fit(X)\n",
    "    labels = np.array(cluster.labels_)+1 \n",
    "\n",
    "    # расчитываем метрику для отдельного слова\n",
    "    ARI.append(adjusted_rand_score(grouped_df.get_group(key)['gold_sense_id'], labels))\n",
    "    \n",
    "print(np.mean(ARI)) # усредненная метрика"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "336b7b38",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Dmitry\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1416: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  super()._check_params_vs_input(X, default_n_init=10)\n",
      "c:\\Users\\Dmitry\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1416: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  super()._check_params_vs_input(X, default_n_init=10)\n",
      "c:\\Users\\Dmitry\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1416: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  super()._check_params_vs_input(X, default_n_init=10)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.06159488872721862\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Dmitry\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1416: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  super()._check_params_vs_input(X, default_n_init=10)\n"
     ]
    }
   ],
   "source": [
    "ARI = []\n",
    "\n",
    "for key, _ in grouped_df:\n",
    "    # вытаскиваем контексты\n",
    "    texts = grouped_df.get_group(key)['context'].values\n",
    "\n",
    "    # создаем пустую матрицу для векторов \n",
    "    X = np.zeros((len(texts), 768))\n",
    "\n",
    "    # переводим тексты в векторы и кладем в матрицу\n",
    "    for i, text in enumerate(texts):\n",
    "        X[i] = embed(text)\n",
    "\n",
    "    # выбираем один из алгоритмов\n",
    "    # cluster = AffinityPropagation(damping=0.9)\n",
    "    cluster = KMeans(7)\n",
    "    # cluster = DBSCAN(min_samples=1, eps=0.1)\n",
    "    \n",
    "    cluster.fit(X)\n",
    "    labels = np.array(cluster.labels_)+1 \n",
    "\n",
    "    # расчитываем метрику для отдельного слова\n",
    "    ARI.append(adjusted_rand_score(grouped_df.get_group(key)['gold_sense_id'], labels))\n",
    "    \n",
    "print(np.mean(ARI)) # усредненная метрика"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "14e3e462",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Dmitry\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1416: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  super()._check_params_vs_input(X, default_n_init=10)\n",
      "c:\\Users\\Dmitry\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1416: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  super()._check_params_vs_input(X, default_n_init=10)\n",
      "c:\\Users\\Dmitry\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1416: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  super()._check_params_vs_input(X, default_n_init=10)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.04013704407686531\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Dmitry\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1416: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  super()._check_params_vs_input(X, default_n_init=10)\n"
     ]
    }
   ],
   "source": [
    "ARI = []\n",
    "\n",
    "for key, _ in grouped_df:\n",
    "    # вытаскиваем контексты\n",
    "    texts = grouped_df.get_group(key)['context'].values\n",
    "\n",
    "    # создаем пустую матрицу для векторов \n",
    "    X = np.zeros((len(texts), 768))\n",
    "\n",
    "    # переводим тексты в векторы и кладем в матрицу\n",
    "    for i, text in enumerate(texts):\n",
    "        X[i] = embed(text)\n",
    "\n",
    "    # выбираем один из алгоритмов\n",
    "    # cluster = AffinityPropagation(damping=0.9)\n",
    "    cluster = KMeans(6)\n",
    "    # cluster = DBSCAN(min_samples=1, eps=0.1)\n",
    "    \n",
    "    cluster.fit(X)\n",
    "    labels = np.array(cluster.labels_)+1 \n",
    "\n",
    "    # расчитываем метрику для отдельного слова\n",
    "    ARI.append(adjusted_rand_score(grouped_df.get_group(key)['gold_sense_id'], labels))\n",
    "    \n",
    "print(np.mean(ARI)) # усредненная метрика"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b206ad00",
   "metadata": {},
   "source": [
    "Смотрим AffinityPropagation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9df184b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.05297560306165972\n"
     ]
    }
   ],
   "source": [
    "ARI = []\n",
    "\n",
    "for key, _ in grouped_df:\n",
    "    # вытаскиваем контексты\n",
    "    texts = grouped_df.get_group(key)['context'].values\n",
    "\n",
    "    # создаем пустую матрицу для векторов \n",
    "    X = np.zeros((len(texts), 768))\n",
    "\n",
    "    # переводим тексты в векторы и кладем в матрицу\n",
    "    for i, text in enumerate(texts):\n",
    "        X[i] = embed(text)\n",
    "\n",
    "    # выбираем один из алгоритмов\n",
    "    cluster = AffinityPropagation(damping=0.9)\n",
    "    #cluster = KMeans(8)\n",
    "    # cluster = DBSCAN(min_samples=1, eps=0.1)\n",
    "    \n",
    "    cluster.fit(X)\n",
    "    labels = np.array(cluster.labels_)+1 \n",
    "\n",
    "    # расчитываем метрику для отдельного слова\n",
    "    ARI.append(adjusted_rand_score(grouped_df.get_group(key)['gold_sense_id'], labels))\n",
    "    \n",
    "print(np.mean(ARI)) # усредненная метрика"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4d2586a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.042740969848549505\n"
     ]
    }
   ],
   "source": [
    "ARI = []\n",
    "\n",
    "for key, _ in grouped_df:\n",
    "    # вытаскиваем контексты\n",
    "    texts = grouped_df.get_group(key)['context'].values\n",
    "\n",
    "    # создаем пустую матрицу для векторов \n",
    "    X = np.zeros((len(texts), 768))\n",
    "\n",
    "    # переводим тексты в векторы и кладем в матрицу\n",
    "    for i, text in enumerate(texts):\n",
    "        X[i] = embed(text)\n",
    "\n",
    "    # выбираем один из алгоритмов\n",
    "    cluster = AffinityPropagation(damping=0.5, max_iter=200, convergence_iter=15)\n",
    "    #cluster = KMeans(8)\n",
    "    # cluster = DBSCAN(min_samples=1, eps=0.1)\n",
    "    \n",
    "    cluster.fit(X)\n",
    "    labels = np.array(cluster.labels_)+1 \n",
    "\n",
    "    # расчитываем метрику для отдельного слова\n",
    "    ARI.append(adjusted_rand_score(grouped_df.get_group(key)['gold_sense_id'], labels))\n",
    "    \n",
    "print(np.mean(ARI)) # усредненная метрика"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "243cfcd9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.04150923434928373\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Dmitry\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\cluster\\_affinity_propagation.py:142: ConvergenceWarning: Affinity propagation did not converge, this model may return degenerate cluster centers and labels.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "ARI = []\n",
    "\n",
    "for key, _ in grouped_df:\n",
    "    # вытаскиваем контексты\n",
    "    texts = grouped_df.get_group(key)['context'].values\n",
    "\n",
    "    # создаем пустую матрицу для векторов \n",
    "    X = np.zeros((len(texts), 768))\n",
    "\n",
    "    # переводим тексты в векторы и кладем в матрицу\n",
    "    for i, text in enumerate(texts):\n",
    "        X[i] = embed(text)\n",
    "\n",
    "    # выбираем один из алгоритмов\n",
    "    cluster = AffinityPropagation(damping=0.7, max_iter=100, convergence_iter=15)\n",
    "    #cluster = KMeans(8)\n",
    "    # cluster = DBSCAN(min_samples=1, eps=0.1)\n",
    "    \n",
    "    cluster.fit(X)\n",
    "    labels = np.array(cluster.labels_)+1 \n",
    "\n",
    "    # расчитываем метрику для отдельного слова\n",
    "    ARI.append(adjusted_rand_score(grouped_df.get_group(key)['gold_sense_id'], labels))\n",
    "    \n",
    "print(np.mean(ARI)) # усредненная метрика"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c864bac7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.04150923434928373\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Dmitry\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\cluster\\_affinity_propagation.py:142: ConvergenceWarning: Affinity propagation did not converge, this model may return degenerate cluster centers and labels.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "ARI = []\n",
    "\n",
    "for key, _ in grouped_df:\n",
    "    # вытаскиваем контексты\n",
    "    texts = grouped_df.get_group(key)['context'].values\n",
    "\n",
    "    # создаем пустую матрицу для векторов \n",
    "    X = np.zeros((len(texts), 768))\n",
    "\n",
    "    # переводим тексты в векторы и кладем в матрицу\n",
    "    for i, text in enumerate(texts):\n",
    "        X[i] = embed(text)\n",
    "\n",
    "    # выбираем один из алгоритмов\n",
    "    cluster = AffinityPropagation(damping=0.7, max_iter=100, convergence_iter=10)\n",
    "    #cluster = KMeans(8)\n",
    "    # cluster = DBSCAN(min_samples=1, eps=0.1)\n",
    "    \n",
    "    cluster.fit(X)\n",
    "    labels = np.array(cluster.labels_)+1 \n",
    "\n",
    "    # расчитываем метрику для отдельного слова\n",
    "    ARI.append(adjusted_rand_score(grouped_df.get_group(key)['gold_sense_id'], labels))\n",
    "    \n",
    "print(np.mean(ARI)) # усредненная метрика"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "cdb38a9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.040604690088052654\n"
     ]
    }
   ],
   "source": [
    "ARI = []\n",
    "\n",
    "for key, _ in grouped_df:\n",
    "    # вытаскиваем контексты\n",
    "    texts = grouped_df.get_group(key)['context'].values\n",
    "\n",
    "    # создаем пустую матрицу для векторов \n",
    "    X = np.zeros((len(texts), 768))\n",
    "\n",
    "    # переводим тексты в векторы и кладем в матрицу\n",
    "    for i, text in enumerate(texts):\n",
    "        X[i] = embed(text)\n",
    "\n",
    "    # выбираем один из алгоритмов\n",
    "    cluster = AffinityPropagation(damping=0.9, max_iter=100, convergence_iter=10)\n",
    "    #cluster = KMeans(8)\n",
    "    # cluster = DBSCAN(min_samples=1, eps=0.1)\n",
    "    \n",
    "    cluster.fit(X)\n",
    "    labels = np.array(cluster.labels_)+1 \n",
    "\n",
    "    # расчитываем метрику для отдельного слова\n",
    "    ARI.append(adjusted_rand_score(grouped_df.get_group(key)['gold_sense_id'], labels))\n",
    "    \n",
    "print(np.mean(ARI)) # усредненная метрика"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97eda096",
   "metadata": {},
   "source": [
    "Смотрим DBSCAN\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b98adae7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.001053019960000099\n"
     ]
    }
   ],
   "source": [
    "ARI = []\n",
    "\n",
    "for key, _ in grouped_df:\n",
    "    # вытаскиваем контексты\n",
    "    texts = grouped_df.get_group(key)['context'].values\n",
    "\n",
    "    # создаем пустую матрицу для векторов \n",
    "    X = np.zeros((len(texts), 768))\n",
    "\n",
    "    # переводим тексты в векторы и кладем в матрицу\n",
    "    for i, text in enumerate(texts):\n",
    "        X[i] = embed(text)\n",
    "\n",
    "    # выбираем один из алгоритмов\n",
    "    #cluster = AffinityPropagation(damping=0.9, max_iter=100, convergence_iter=10)\n",
    "    #cluster = KMeans(8)\n",
    "    cluster = DBSCAN(min_samples=1, eps=0.1)\n",
    "    \n",
    "    cluster.fit(X)\n",
    "    labels = np.array(cluster.labels_)+1 \n",
    "\n",
    "    # расчитываем метрику для отдельного слова\n",
    "    ARI.append(adjusted_rand_score(grouped_df.get_group(key)['gold_sense_id'], labels))\n",
    "    \n",
    "print(np.mean(ARI)) # усредненная метрика"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "54bc21e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-0.02349153051130162\n"
     ]
    }
   ],
   "source": [
    "ARI = []\n",
    "\n",
    "for key, _ in grouped_df:\n",
    "    # вытаскиваем контексты\n",
    "    texts = grouped_df.get_group(key)['context'].values\n",
    "\n",
    "    # создаем пустую матрицу для векторов \n",
    "    X = np.zeros((len(texts), 768))\n",
    "\n",
    "    # переводим тексты в векторы и кладем в матрицу\n",
    "    for i, text in enumerate(texts):\n",
    "        X[i] = embed(text)\n",
    "\n",
    "    # выбираем один из алгоритмов\n",
    "    #cluster = AffinityPropagation(damping=0.9, max_iter=100, convergence_iter=10)\n",
    "    #cluster = KMeans(8)\n",
    "    cluster = DBSCAN(eps=0.3, min_samples=5, metric='euclidean', algorithm='auto', leaf_size=30)\n",
    "    \n",
    "    cluster.fit(X)\n",
    "    labels = np.array(cluster.labels_)+1 \n",
    "\n",
    "    # расчитываем метрику для отдельного слова\n",
    "    ARI.append(adjusted_rand_score(grouped_df.get_group(key)['gold_sense_id'], labels))\n",
    "    \n",
    "print(np.mean(ARI)) # усредненная метрика"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "3651e823",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0\n"
     ]
    }
   ],
   "source": [
    "ARI = []\n",
    "\n",
    "for key, _ in grouped_df:\n",
    "    # вытаскиваем контексты\n",
    "    texts = grouped_df.get_group(key)['context'].values\n",
    "\n",
    "    # создаем пустую матрицу для векторов \n",
    "    X = np.zeros((len(texts), 768))\n",
    "\n",
    "    # переводим тексты в векторы и кладем в матрицу\n",
    "    for i, text in enumerate(texts):\n",
    "        X[i] = embed(text)\n",
    "\n",
    "    # выбираем один из алгоритмов\n",
    "    #cluster = AffinityPropagation(damping=0.9, max_iter=100, convergence_iter=10)\n",
    "    #cluster = KMeans(8)\n",
    "    cluster = DBSCAN(eps=0.3, min_samples=5, metric='manhattan')\n",
    "    \n",
    "    cluster.fit(X)\n",
    "    labels = np.array(cluster.labels_)+1 \n",
    "\n",
    "    # расчитываем метрику для отдельного слова\n",
    "    ARI.append(adjusted_rand_score(grouped_df.get_group(key)['gold_sense_id'], labels))\n",
    "    \n",
    "print(np.mean(ARI)) # усредненная метрика"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "e2f65b23",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-0.001784882962278153\n"
     ]
    }
   ],
   "source": [
    "ARI = []\n",
    "\n",
    "for key, _ in grouped_df:\n",
    "    # вытаскиваем контексты\n",
    "    texts = grouped_df.get_group(key)['context'].values\n",
    "\n",
    "    # создаем пустую матрицу для векторов \n",
    "    X = np.zeros((len(texts), 768))\n",
    "\n",
    "    # переводим тексты в векторы и кладем в матрицу\n",
    "    for i, text in enumerate(texts):\n",
    "        X[i] = embed(text)\n",
    "\n",
    "    # выбираем один из алгоритмов\n",
    "    #cluster = AffinityPropagation(damping=0.9, max_iter=100, convergence_iter=10)\n",
    "    #cluster = KMeans(8)\n",
    "    cluster = DBSCAN(eps=0.3, min_samples=5, metric='cosine')\n",
    "    \n",
    "    cluster.fit(X)\n",
    "    labels = np.array(cluster.labels_)+1 \n",
    "\n",
    "    # расчитываем метрику для отдельного слова\n",
    "    ARI.append(adjusted_rand_score(grouped_df.get_group(key)['gold_sense_id'], labels))\n",
    "    \n",
    "print(np.mean(ARI)) # усредненная метрика"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "cbd182ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.00443342207693399\n"
     ]
    }
   ],
   "source": [
    "ARI = []\n",
    "\n",
    "for key, _ in grouped_df:\n",
    "    # вытаскиваем контексты\n",
    "    texts = grouped_df.get_group(key)['context'].values\n",
    "\n",
    "    # создаем пустую матрицу для векторов \n",
    "    X = np.zeros((len(texts), 768))\n",
    "\n",
    "    # переводим тексты в векторы и кладем в матрицу\n",
    "    for i, text in enumerate(texts):\n",
    "        X[i] = embed(text)\n",
    "\n",
    "    # выбираем один из алгоритмов\n",
    "    #cluster = AffinityPropagation(damping=0.9, max_iter=100, convergence_iter=10)\n",
    "    #cluster = KMeans(8)\n",
    "    cluster = DBSCAN(eps=0.2, min_samples=1, metric='euclidean')\n",
    "    \n",
    "    cluster.fit(X)\n",
    "    labels = np.array(cluster.labels_)+1 \n",
    "\n",
    "    # расчитываем метрику для отдельного слова\n",
    "    ARI.append(adjusted_rand_score(grouped_df.get_group(key)['gold_sense_id'], labels))\n",
    "    \n",
    "print(np.mean(ARI)) # усредненная метрика"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dfdf2a8",
   "metadata": {},
   "source": [
    "Смотрим на Agglomerative clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "0d96ad00",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import AgglomerativeClustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "4bef8754",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Dmitry\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\cluster\\_agglomerative.py:1006: FutureWarning: Attribute `affinity` was deprecated in version 1.2 and will be removed in 1.4. Use `metric` instead\n",
      "  warnings.warn(\n",
      "c:\\Users\\Dmitry\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\cluster\\_agglomerative.py:1006: FutureWarning: Attribute `affinity` was deprecated in version 1.2 and will be removed in 1.4. Use `metric` instead\n",
      "  warnings.warn(\n",
      "c:\\Users\\Dmitry\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\cluster\\_agglomerative.py:1006: FutureWarning: Attribute `affinity` was deprecated in version 1.2 and will be removed in 1.4. Use `metric` instead\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.034738071269679226\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Dmitry\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\cluster\\_agglomerative.py:1006: FutureWarning: Attribute `affinity` was deprecated in version 1.2 and will be removed in 1.4. Use `metric` instead\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "ARI = []\n",
    "\n",
    "for key, _ in grouped_df:\n",
    "    # вытаскиваем контексты\n",
    "    texts = grouped_df.get_group(key)['context'].values\n",
    "\n",
    "    # создаем пустую матрицу для векторов \n",
    "    X = np.zeros((len(texts), 768))\n",
    "\n",
    "    # переводим тексты в векторы и кладем в матрицу\n",
    "    for i, text in enumerate(texts):\n",
    "        X[i] = embed(text)\n",
    "\n",
    "    # выбираем один из алгоритмов\n",
    "    #cluster = AffinityPropagation(damping=0.9, max_iter=100, convergence_iter=10)\n",
    "    #cluster = KMeans(8)\n",
    "    #cluster = DBSCAN(eps=0.3, min_samples=5, metric='euclidean')\n",
    "    cluster = AgglomerativeClustering(n_clusters=3, linkage='ward', affinity='euclidean')\n",
    "\n",
    "    \n",
    "    cluster.fit(X)\n",
    "    labels = np.array(cluster.labels_)+1 \n",
    "\n",
    "    # расчитываем метрику для отдельного слова\n",
    "    ARI.append(adjusted_rand_score(grouped_df.get_group(key)['gold_sense_id'], labels))\n",
    "    \n",
    "print(np.mean(ARI)) # усредненная метрика"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "4f012356",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Dmitry\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\cluster\\_agglomerative.py:1006: FutureWarning: Attribute `affinity` was deprecated in version 1.2 and will be removed in 1.4. Use `metric` instead\n",
      "  warnings.warn(\n",
      "c:\\Users\\Dmitry\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\cluster\\_agglomerative.py:1006: FutureWarning: Attribute `affinity` was deprecated in version 1.2 and will be removed in 1.4. Use `metric` instead\n",
      "  warnings.warn(\n",
      "c:\\Users\\Dmitry\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\cluster\\_agglomerative.py:1006: FutureWarning: Attribute `affinity` was deprecated in version 1.2 and will be removed in 1.4. Use `metric` instead\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.11591119562417704\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Dmitry\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\cluster\\_agglomerative.py:1006: FutureWarning: Attribute `affinity` was deprecated in version 1.2 and will be removed in 1.4. Use `metric` instead\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "ARI = []\n",
    "\n",
    "for key, _ in grouped_df:\n",
    "    # вытаскиваем контексты\n",
    "    texts = grouped_df.get_group(key)['context'].values\n",
    "\n",
    "    # создаем пустую матрицу для векторов \n",
    "    X = np.zeros((len(texts), 768))\n",
    "\n",
    "    # переводим тексты в векторы и кладем в матрицу\n",
    "    for i, text in enumerate(texts):\n",
    "        X[i] = embed(text)\n",
    "\n",
    "    # выбираем один из алгоритмов\n",
    "    #cluster = AffinityPropagation(damping=0.9, max_iter=100, convergence_iter=10)\n",
    "    #cluster = KMeans(8)\n",
    "    #cluster = DBSCAN(eps=0.3, min_samples=5, metric='euclidean')\n",
    "    cluster = AgglomerativeClustering(n_clusters=3, linkage='complete', affinity='euclidean')\n",
    "\n",
    "    \n",
    "    cluster.fit(X)\n",
    "    labels = np.array(cluster.labels_)+1 \n",
    "\n",
    "    # расчитываем метрику для отдельного слова\n",
    "    ARI.append(adjusted_rand_score(grouped_df.get_group(key)['gold_sense_id'], labels))\n",
    "    \n",
    "print(np.mean(ARI)) # усредненная метрика"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "b3b192ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Dmitry\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\cluster\\_agglomerative.py:1006: FutureWarning: Attribute `affinity` was deprecated in version 1.2 and will be removed in 1.4. Use `metric` instead\n",
      "  warnings.warn(\n",
      "c:\\Users\\Dmitry\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\cluster\\_agglomerative.py:1006: FutureWarning: Attribute `affinity` was deprecated in version 1.2 and will be removed in 1.4. Use `metric` instead\n",
      "  warnings.warn(\n",
      "c:\\Users\\Dmitry\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\cluster\\_agglomerative.py:1006: FutureWarning: Attribute `affinity` was deprecated in version 1.2 and will be removed in 1.4. Use `metric` instead\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-0.009158236277409101\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Dmitry\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\cluster\\_agglomerative.py:1006: FutureWarning: Attribute `affinity` was deprecated in version 1.2 and will be removed in 1.4. Use `metric` instead\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "ARI = []\n",
    "\n",
    "for key, _ in grouped_df:\n",
    "    # вытаскиваем контексты\n",
    "    texts = grouped_df.get_group(key)['context'].values\n",
    "\n",
    "    # создаем пустую матрицу для векторов \n",
    "    X = np.zeros((len(texts), 768))\n",
    "\n",
    "    # переводим тексты в векторы и кладем в матрицу\n",
    "    for i, text in enumerate(texts):\n",
    "        X[i] = embed(text)\n",
    "\n",
    "    # выбираем один из алгоритмов\n",
    "    #cluster = AffinityPropagation(damping=0.9, max_iter=100, convergence_iter=10)\n",
    "    #cluster = KMeans(8)\n",
    "    #cluster = DBSCAN(eps=0.3, min_samples=5, metric='euclidean')\n",
    "    cluster = AgglomerativeClustering(n_clusters=3, linkage='average', affinity='euclidean')\n",
    "\n",
    "    \n",
    "    cluster.fit(X)\n",
    "    labels = np.array(cluster.labels_)+1 \n",
    "\n",
    "    # расчитываем метрику для отдельного слова\n",
    "    ARI.append(adjusted_rand_score(grouped_df.get_group(key)['gold_sense_id'], labels))\n",
    "    \n",
    "print(np.mean(ARI)) # усредненная метрика"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "601f8a6f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Dmitry\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\cluster\\_agglomerative.py:1006: FutureWarning: Attribute `affinity` was deprecated in version 1.2 and will be removed in 1.4. Use `metric` instead\n",
      "  warnings.warn(\n",
      "c:\\Users\\Dmitry\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\cluster\\_agglomerative.py:1006: FutureWarning: Attribute `affinity` was deprecated in version 1.2 and will be removed in 1.4. Use `metric` instead\n",
      "  warnings.warn(\n",
      "c:\\Users\\Dmitry\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\cluster\\_agglomerative.py:1006: FutureWarning: Attribute `affinity` was deprecated in version 1.2 and will be removed in 1.4. Use `metric` instead\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0519783194524109\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Dmitry\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\cluster\\_agglomerative.py:1006: FutureWarning: Attribute `affinity` was deprecated in version 1.2 and will be removed in 1.4. Use `metric` instead\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "ARI = []\n",
    "\n",
    "for key, _ in grouped_df:\n",
    "    # вытаскиваем контексты\n",
    "    texts = grouped_df.get_group(key)['context'].values\n",
    "\n",
    "    # создаем пустую матрицу для векторов \n",
    "    X = np.zeros((len(texts), 768))\n",
    "\n",
    "    # переводим тексты в векторы и кладем в матрицу\n",
    "    for i, text in enumerate(texts):\n",
    "        X[i] = embed(text)\n",
    "\n",
    "    # выбираем один из алгоритмов\n",
    "    #cluster = AffinityPropagation(damping=0.9, max_iter=100, convergence_iter=10)\n",
    "    #cluster = KMeans(8)\n",
    "    #cluster = DBSCAN(eps=0.3, min_samples=5, metric='euclidean')\n",
    "    cluster = AgglomerativeClustering(n_clusters=3, linkage='single', affinity='euclidean')\n",
    "\n",
    "    \n",
    "    cluster.fit(X)\n",
    "    labels = np.array(cluster.labels_)+1 \n",
    "\n",
    "    # расчитываем метрику для отдельного слова\n",
    "    ARI.append(adjusted_rand_score(grouped_df.get_group(key)['gold_sense_id'], labels))\n",
    "    \n",
    "print(np.mean(ARI)) # усредненная метрика"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "41bfc3b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Dmitry\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\cluster\\_agglomerative.py:1006: FutureWarning: Attribute `affinity` was deprecated in version 1.2 and will be removed in 1.4. Use `metric` instead\n",
      "  warnings.warn(\n",
      "c:\\Users\\Dmitry\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\cluster\\_agglomerative.py:1006: FutureWarning: Attribute `affinity` was deprecated in version 1.2 and will be removed in 1.4. Use `metric` instead\n",
      "  warnings.warn(\n",
      "c:\\Users\\Dmitry\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\cluster\\_agglomerative.py:1006: FutureWarning: Attribute `affinity` was deprecated in version 1.2 and will be removed in 1.4. Use `metric` instead\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0716318676086122\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Dmitry\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\cluster\\_agglomerative.py:1006: FutureWarning: Attribute `affinity` was deprecated in version 1.2 and will be removed in 1.4. Use `metric` instead\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "ARI = []\n",
    "\n",
    "for key, _ in grouped_df:\n",
    "    # вытаскиваем контексты\n",
    "    texts = grouped_df.get_group(key)['context'].values\n",
    "\n",
    "    # создаем пустую матрицу для векторов \n",
    "    X = np.zeros((len(texts), 768))\n",
    "\n",
    "    # переводим тексты в векторы и кладем в матрицу\n",
    "    for i, text in enumerate(texts):\n",
    "        X[i] = embed(text)\n",
    "\n",
    "    # выбираем один из алгоритмов\n",
    "    #cluster = AffinityPropagation(damping=0.9, max_iter=100, convergence_iter=10)\n",
    "    #cluster = KMeans(8)\n",
    "    #cluster = DBSCAN(eps=0.3, min_samples=5, metric='euclidean')\n",
    "    cluster = AgglomerativeClustering(n_clusters=10, linkage='complete', affinity='euclidean')\n",
    "\n",
    "    \n",
    "    cluster.fit(X)\n",
    "    labels = np.array(cluster.labels_)+1 \n",
    "\n",
    "    # расчитываем метрику для отдельного слова\n",
    "    ARI.append(adjusted_rand_score(grouped_df.get_group(key)['gold_sense_id'], labels))\n",
    "    \n",
    "print(np.mean(ARI)) # усредненная метрика"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f329223a",
   "metadata": {},
   "source": [
    "Смотрим на пятый алгоритм на выбор - Spectral clustering\n",
    "\n",
    "Как работает? Создав матрицу, алгоритм упрощает её, уменьшая количество измерений, но сохраняя более релевантные параметры. Потом, алгоритм делит элементы на заданное число кластеров. Инструкция уверяет, что данный алгоритм особенно хорошо работает с делением на два кластера. Он пытается разделить данные на две группы таким образом, чтобы минимизировать связи между группами и максимизировать связи внутри каждой группы."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "8f84374a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import SpectralClustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "ca25165e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.029294289618017777\n"
     ]
    }
   ],
   "source": [
    "ARI = []\n",
    "\n",
    "for key, _ in grouped_df:\n",
    "    # вытаскиваем контексты\n",
    "    texts = grouped_df.get_group(key)['context'].values\n",
    "\n",
    "    # создаем пустую матрицу для векторов \n",
    "    X = np.zeros((len(texts), 768))\n",
    "\n",
    "    # переводим тексты в векторы и кладем в матрицу\n",
    "    for i, text in enumerate(texts):\n",
    "        X[i] = embed(text)\n",
    "\n",
    "    # выбираем один из алгоритмов\n",
    "    #cluster = AffinityPropagation(damping=0.9, max_iter=100, convergence_iter=10)\n",
    "    #cluster = KMeans(8)\n",
    "    #cluster = DBSCAN(eps=0.2, min_samples=1, metric='euclidean')\n",
    "    cluster = SpectralClustering(n_clusters=2)\n",
    "    \n",
    "    cluster.fit(X)\n",
    "    labels = np.array(cluster.labels_)+1 \n",
    "\n",
    "    # расчитываем метрику для отдельного слова\n",
    "    ARI.append(adjusted_rand_score(grouped_df.get_group(key)['gold_sense_id'], labels))\n",
    "    \n",
    "print(np.mean(ARI)) # усредненная метрика"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "6ce8634b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.03880940196533622\n"
     ]
    }
   ],
   "source": [
    "ARI = []\n",
    "\n",
    "for key, _ in grouped_df:\n",
    "    # вытаскиваем контексты\n",
    "    texts = grouped_df.get_group(key)['context'].values\n",
    "\n",
    "    # создаем пустую матрицу для векторов \n",
    "    X = np.zeros((len(texts), 768))\n",
    "\n",
    "    # переводим тексты в векторы и кладем в матрицу\n",
    "    for i, text in enumerate(texts):\n",
    "        X[i] = embed(text)\n",
    "\n",
    "    # выбираем один из алгоритмов\n",
    "    #cluster = AffinityPropagation(damping=0.9, max_iter=100, convergence_iter=10)\n",
    "    #cluster = KMeans(8)\n",
    "    #cluster = DBSCAN(eps=0.2, min_samples=1, metric='euclidean')\n",
    "    cluster = SpectralClustering(n_clusters=2, affinity='nearest_neighbors')\n",
    "    \n",
    "    cluster.fit(X)\n",
    "    labels = np.array(cluster.labels_)+1 \n",
    "\n",
    "    # расчитываем метрику для отдельного слова\n",
    "    ARI.append(adjusted_rand_score(grouped_df.get_group(key)['gold_sense_id'], labels))\n",
    "    \n",
    "print(np.mean(ARI)) # усредненная метрика"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "2f17c902",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.057696021574798674\n"
     ]
    }
   ],
   "source": [
    "ARI = []\n",
    "\n",
    "for key, _ in grouped_df:\n",
    "    # вытаскиваем контексты\n",
    "    texts = grouped_df.get_group(key)['context'].values\n",
    "\n",
    "    # создаем пустую матрицу для векторов \n",
    "    X = np.zeros((len(texts), 768))\n",
    "\n",
    "    # переводим тексты в векторы и кладем в матрицу\n",
    "    for i, text in enumerate(texts):\n",
    "        X[i] = embed(text)\n",
    "\n",
    "    # выбираем один из алгоритмов\n",
    "    #cluster = AffinityPropagation(damping=0.9, max_iter=100, convergence_iter=10)\n",
    "    #cluster = KMeans(8)\n",
    "    #cluster = DBSCAN(eps=0.2, min_samples=1, metric='euclidean')\n",
    "    cluster = SpectralClustering(n_clusters=3)\n",
    "    \n",
    "    cluster.fit(X)\n",
    "    labels = np.array(cluster.labels_)+1 \n",
    "\n",
    "    # расчитываем метрику для отдельного слова\n",
    "    ARI.append(adjusted_rand_score(grouped_df.get_group(key)['gold_sense_id'], labels))\n",
    "    \n",
    "print(np.mean(ARI)) # усредненная метрика"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "31f772ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.08339604125532275\n"
     ]
    }
   ],
   "source": [
    "ARI = []\n",
    "\n",
    "for key, _ in grouped_df:\n",
    "    # вытаскиваем контексты\n",
    "    texts = grouped_df.get_group(key)['context'].values\n",
    "\n",
    "    # создаем пустую матрицу для векторов \n",
    "    X = np.zeros((len(texts), 768))\n",
    "\n",
    "    # переводим тексты в векторы и кладем в матрицу\n",
    "    for i, text in enumerate(texts):\n",
    "        X[i] = embed(text)\n",
    "\n",
    "    # выбираем один из алгоритмов\n",
    "    #cluster = AffinityPropagation(damping=0.9, max_iter=100, convergence_iter=10)\n",
    "    #cluster = KMeans(8)\n",
    "    #cluster = DBSCAN(eps=0.2, min_samples=1, metric='euclidean')\n",
    "    cluster = SpectralClustering(n_clusters=3, affinity='nearest_neighbors')\n",
    "    \n",
    "    cluster.fit(X)\n",
    "    labels = np.array(cluster.labels_)+1 \n",
    "\n",
    "    # расчитываем метрику для отдельного слова\n",
    "    ARI.append(adjusted_rand_score(grouped_df.get_group(key)['gold_sense_id'], labels))\n",
    "    \n",
    "print(np.mean(ARI)) # усредненная метрика"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "b26232b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.06722011006535526\n"
     ]
    }
   ],
   "source": [
    "ARI = []\n",
    "\n",
    "for key, _ in grouped_df:\n",
    "    # вытаскиваем контексты\n",
    "    texts = grouped_df.get_group(key)['context'].values\n",
    "\n",
    "    # создаем пустую матрицу для векторов \n",
    "    X = np.zeros((len(texts), 768))\n",
    "\n",
    "    # переводим тексты в векторы и кладем в матрицу\n",
    "    for i, text in enumerate(texts):\n",
    "        X[i] = embed(text)\n",
    "\n",
    "    # выбираем один из алгоритмов\n",
    "    #cluster = AffinityPropagation(damping=0.9, max_iter=100, convergence_iter=10)\n",
    "    #cluster = KMeans(8)\n",
    "    #cluster = DBSCAN(eps=0.2, min_samples=1, metric='euclidean')\n",
    "    cluster = SpectralClustering(n_clusters=5)\n",
    "    \n",
    "    cluster.fit(X)\n",
    "    labels = np.array(cluster.labels_)+1 \n",
    "\n",
    "    # расчитываем метрику для отдельного слова\n",
    "    ARI.append(adjusted_rand_score(grouped_df.get_group(key)['gold_sense_id'], labels))\n",
    "    \n",
    "print(np.mean(ARI)) # усредненная метрика"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
