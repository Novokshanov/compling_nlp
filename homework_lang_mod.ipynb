{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "00fad453",
   "metadata": {},
   "source": [
    "# Домашнее задание № 4. Языковые модели"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d056af4",
   "metadata": {},
   "source": [
    "## Задание 1 (8 баллов)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1f532a8",
   "metadata": {},
   "source": [
    "В семинаре для генерации мы использовали предположение маркова и считали, что слово зависит только от 1 предыдущего слова. Но ничто нам не мешает попробовать увеличить размер окна и учитывать два или даже три прошлых слова. Для них мы еще сможем собрать достаточно статистик и, логично предположить, что качество сгенерированного текста должно вырасти."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de743d1d",
   "metadata": {},
   "source": [
    "Попробуйте сделать языковую модель, которая будет учитывать два предыдущих слова при генерации текста.\n",
    "Сгенерируйте несколько текстов (3-5) и расчитайте перплексию получившейся модели. \n",
    "Можно использовать данные из семинара или любые другие (можно брать только часть текста, если считается слишком долго). Перплексию рассчитывайте на 10-50 отложенных предложениях (они не должны использоваться при сборе статистик).\n",
    "\n",
    "\n",
    "Подсказки:  \n",
    "    - нужно будет добавить еще один тэг \\<start>  \n",
    "    - можете использовать тот же подход с матрицей вероятностей, но по строкам хранить биграмы, а по колонкам униграммы \n",
    "    - тексты должны быть очень похожи на нормальные (если у вас получается рандомная каша, вы что-то делаете не так)\n",
    "    - у вас будут словари с индексами биграммов и униграммов, не перепутайте их при переводе индекса в слово - словарь биграммов будет больше словаря униграммов и все индексы из униграммного словаря будут формально подходить для словаря биграммов (не будет ошибки при id2bigram[unigram_id]), но маппинг при этом будет совершенно неправильным "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d078056d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from string import punctuation\n",
    "from razdel import sentenize\n",
    "from razdel import tokenize as razdel_tokenize\n",
    "import numpy as np\n",
    "from IPython.display import Image\n",
    "from IPython.core.display import HTML\n",
    "from collections import Counter\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from scipy.sparse import lil_matrix, csr_matrix, csc_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6afcef88",
   "metadata": {},
   "outputs": [],
   "source": [
    "#with open('data/2ch_corpus.txt', 'r', encoding='utf-8') as file1:\n",
    "#    dvach = file1.read()\n",
    "\n",
    "with open('data/lenta.txt', 'r', encoding='utf-8') as file2:\n",
    "    news = file2.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize(text):\n",
    "    normalized_text = [word.text.strip(punctuation) for word \\\n",
    "                                                            in razdel_tokenize(text)]\n",
    "    normalized_text = [word.lower() for word in normalized_text if word and len(word) < 20 ]\n",
    "    return normalized_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ngrammer(tokens, n=2):\n",
    "    ngrams = []\n",
    "    for i in range(0,len(tokens)-n+1):\n",
    "        ngrams.append(' '.join(tokens[i:i+n]))\n",
    "    return ngrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sentences_dvach = [['<start>'] + ['<start>'] + normalize(text) + ['<end>'] for text in sent_tokenize(dvach[:50000])]\n",
    "sentences_news = [['<start>'] + ['<start>'] + normalize(text) + ['<end>'] for text in sent_tokenize(news[:5000000])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "32250"
      ]
     },
     "execution_count": 217,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(sentences_news)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences_news_check = [['<start>'] + ['<start>'] + normalize(text) + ['<end>'] for text in sent_tokenize(news[5000001:5005000])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "40"
      ]
     },
     "execution_count": 220,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(sentences_news_check)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "#unigrams_dvach = Counter()\n",
    "#bigrams_dvach = Counter()\n",
    "#trigrams_dvach = Counter()\n",
    "\n",
    "#for sentence in sentences_dvach:\n",
    "#    unigrams_dvach.update(sentence)\n",
    "#    bigrams_dvach.update(ngrammer(sentence))\n",
    "#    trigrams_dvach.update(ngrammer(sentence, n=3))\n",
    "\n",
    "\n",
    "unigrams_news = Counter()\n",
    "bigrams_news = Counter()\n",
    "trigrams_news = Counter()\n",
    "\n",
    "for sentence in sentences_news:\n",
    "    unigrams_news.update(sentence)\n",
    "    bigrams_news.update(ngrammer(sentence))\n",
    "    trigrams_news.update(ngrammer(sentence, n=3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [],
   "source": [
    "matrix_news = lil_matrix((len(bigrams_news), \n",
    "                        len(unigrams_news)))\n",
    "\n",
    "id2word_news = list(unigrams_news)\n",
    "word2id_news = {word:i for i, word in enumerate(id2word_news)}\n",
    "id2bigram_news = list(bigrams_news)\n",
    "bigram2id_news = {bigram:i for i, bigram in enumerate(id2bigram_news)}\n",
    "\n",
    "\n",
    "for ngram in trigrams_news:\n",
    "    word1, word2, word3 = ngram.split()\n",
    "    bigram = word1 + ' ' + word2\n",
    "    matrix_news[bigram2id_news[bigram], word2id_news[word3]] =  (trigrams_news[ngram]/\n",
    "                                                                     bigrams_news[bigram])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate(matrix, id2word, bigram2id, n=100, start='<start> <start>'):\n",
    "    text = []\n",
    "    current_idx = bigram2id[start]\n",
    "    \n",
    "    for i in range(n):\n",
    "        row_probabilities = matrix[current_idx].toarray()[0]\n",
    "\n",
    "        # Проверка на NaN или нулевые вероятности\n",
    "        if np.any(np.isnan(row_probabilities)) or row_probabilities.sum() == 0:\n",
    "            chosen = np.random.choice(matrix.shape[1])\n",
    "        else:\n",
    "            # Нормализация вероятностей\n",
    "            row_probabilities /= row_probabilities.sum()\n",
    "            chosen = np.random.choice(matrix.shape[1], p=row_probabilities)\n",
    "\n",
    "        text.append(id2word[chosen])\n",
    "\n",
    "        if id2word[chosen] == '<end>':\n",
    "            chosen = bigram2id['<start> <start>']\n",
    "\n",
    "        words = id2bigram_news[current_idx]\n",
    "        word1, word2 = words.split()\n",
    "        word3 = id2word[chosen]\n",
    "\n",
    "        new_bigram = word2 + ' ' + word3\n",
    "\n",
    "        if new_bigram in bigram2id:\n",
    "            current_idx = bigram2id[new_bigram]\n",
    "        else:\n",
    "            new_bigram = '<start> ' + word3\n",
    "            current_idx = bigram2id[new_bigram]\n",
    "\n",
    "    return ' '.join(text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "другой опрос но и в перспективе так как второй транш кредита в 35 из них 2438 дети до 16 миллионов тройских унций около 500 семей москвичей остались без связи из-за аварии восстановлена по данным российского оборонного ведомства в шали около 150 миллионов рублей \n",
      " окончательную точку здесь поставит суд добавил владимир козлов \n",
      " переговоры скаутов c новыми владельцами домена результатов пока не нашло доказательств существования подобных тайников но свидетельства похожих акций советской разведки были найдены пути решения вопросов связанных с высокими технологиями а также итоги 1999 года на указанной частоте ведется круглосуточная трансляция новой радиопрограммы объявляемой в эфире радиостанции эхо\n"
     ]
    }
   ],
   "source": [
    "print(generate(matrix_news, id2word_news, bigram2id_news).replace('<end>', '\\n'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "газпром является владельцем легкового автомобиля газ-3111 волга \n",
      " член мгик требует через суд или суды за рубежом кправительству и минфину рф полномочия по регистрации выпусков облигаций цб а также новый генеральный секретарь совета безопасности марат тажин распорядился создать новое государство от участия в выборах в госдуму россии о восстановлении регистрации списка федерального избирательного объединения снимет свою кандидатуру на президентских выборах сейчас имеет визу h-1 b и тех кто заказал дело лазаренко кто довел народ украины до сих пор находится под стражей \n",
      " управделами считает что представителя президента рф правительстварф каждой из которых на одного человека· мяса 31,5 кг · молока\n"
     ]
    }
   ],
   "source": [
    "print(generate(matrix_news, id2word_news, bigram2id_news).replace('<end>', '\\n'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 300,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "платить за акции по завышенным ценам \n",
      " полиция началарасследование но никакие подробности пока не установлены \n",
      " крушение произошло недалеко от стамбула \n",
      " напомним что в городе строительство парламентского центра чтобы перевести в федеральное подчинение \n",
      " пожару была присвоена степень почетного доктора \n",
      " подсчет голосов еще продолжается в субботу в дорожную аварию он отделалася лишь несколькими царапинами на голове и проникнув в здание генпрокуратуры и нормальные условия хранения создать в россии альберта давлеева главная цель этого этапа окончательное уничтожение бандформирований на северном кавказе приступает ко второму чтению крупнейший британский банк barclays принял свое решение заключением британских врачей \n",
      " также велось прочесывание\n"
     ]
    }
   ],
   "source": [
    "print(generate(matrix_news, id2word_news, bigram2id_news).replace('<end>', '\\n'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [],
   "source": [
    "def perplexity(logp, N):\n",
    "    return np.exp((-1/N) * logp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = ''\n",
    "\n",
    "for sentence in sentences_news_check:\n",
    "    for word in sentence:\n",
    "        word += ' '\n",
    "        text += word\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {},
   "outputs": [],
   "source": [
    "text1 = normalize(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_joint_proba_markov_assumption(text, word_counts, bigram_counts, trigram_counts):\n",
    "    prob = 0\n",
    "    for ngram in ngrammer(text, n=3):\n",
    "        word1, word2, word3 = ngram.split()\n",
    "        bigram = word1 + ' ' + word2\n",
    "        if bigram in bigram_counts and ngram in trigram_counts:\n",
    "            prob += np.log(trigram_counts[ngram]/bigram_counts[bigram])\n",
    "        else:\n",
    "            prob += np.log(2e-5)\n",
    "    \n",
    "    return prob, len(text1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 301,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15213.303465450957"
      ]
     },
     "execution_count": 301,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "perplexity(*compute_joint_proba_markov_assumption(text1, unigrams_news, bigrams_news, trigrams_news))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e0a8dd5",
   "metadata": {},
   "source": [
    "## Задание № 2* (2 балла). "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f733858c",
   "metadata": {},
   "source": [
    "Измените функцию generate_with_beam_search так, чтобы она работала с моделью, которая учитывает два предыдущих слова. \n",
    "Сравните получаемый результат с первым заданием. \n",
    "Также попробуйте начинать генерацию не с нуля (подавая \\<start> \\<start>), а с какого-то промпта. Но помните, что учитываться будут только два последних слова, так что не делайте длинные промпты."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "id": "c426746a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# сделаем класс чтобы хранить каждый из лучей\n",
    "class Beam:\n",
    "    def __init__(self, sequence: list, score: float):\n",
    "        self.sequence: list = sequence\n",
    "        self.score: float = score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_with_beam_search(matrix, id2word, bigram2id, n=100, max_beams=5, start='<start> <start>'):\n",
    "    # изначально у нас один луч с заданным началом (start по дефолту)\n",
    "    initial_node = Beam(sequence=[start], score=np.log1p(0))\n",
    "    beams = [initial_node]\n",
    "    \n",
    "    for i in range(n):\n",
    "        # делаем n шагов генерации\n",
    "        new_beams = []\n",
    "        # на каждом шаге продолжаем каждый из имеющихся лучей\n",
    "        for beam in beams:\n",
    "            # лучи которые уже закончены не продолжаем (но и не удаляем)\n",
    "            if beam.sequence[-1] == '<end>':\n",
    "                new_beams.append(beam)\n",
    "                continue\n",
    "            \n",
    "            # наша языковая модель предсказывает на основе предыдущего слова\n",
    "            # достанем его из beam.sequence\n",
    "            a = beam.sequence[-2:]\n",
    "            aa = ' '.join(a)\n",
    "            if len(aa) == 1:\n",
    "                aa = '<start> ' + aa\n",
    "            if aa not in bigram2id:\n",
    "                aa = '<start> <start>'\n",
    "\n",
    "            last_id = bigram2id[aa]\n",
    "            \n",
    "            # посмотрим вероятности продолжений для предыдущего слова\n",
    "            probas = matrix[last_id].toarray()[0]\n",
    "            \n",
    "            # возьмем топ самых вероятных продолжений\n",
    "            top_idxs = probas.argsort()[:-(max_beams+1):-1]\n",
    "            for top_id in top_idxs:\n",
    "                # иногда вероятности будут нулевые, такое не добавляем\n",
    "                if not probas[top_id]:\n",
    "                    break\n",
    "                \n",
    "                # создадим новый луч на основе текущего и варианта продолжения\n",
    "                new_sequence = beam.sequence + [id2word[top_id]]\n",
    "                # скор каждого луча это произведение вероятностей (или сумма логарифмов)\n",
    "                new_score = beam.score + np.log1p(probas[top_id])\n",
    "                new_beam = Beam(sequence=new_sequence, score=new_score)\n",
    "                new_beams.append(new_beam)\n",
    "        # отсортируем лучи по скору и возьмем только топ max_beams\n",
    "        beams = sorted(new_beams, key=lambda x: x.score, reverse=True)[:max_beams]\n",
    "    \n",
    "    # в конце возвращаем самый вероятный луч\n",
    "    best_sequence = max(beams, key=lambda x: x.score).sequence\n",
    "\n",
    "    \n",
    "    return ' '.join(best_sequence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "другой чеченский боевик шамиль басаев руслан гелаев ваха арсанов ахмед закаев и другие документы готовятся дляотправки в ближайшее времяможет начаться спецоперация против бандформирований которые терроризируют мирное население и работу предприятий перерабатывающей промышленности санкт-петербурга составил 234 процента по сравнению с прошлым годом когда финансы были расшатаны долги росли и дефицит бюджета достиг 8 процентов от суммы привлеченных валютных депозитов и 7 представителей glaxo и smithkline beecham планируют провести диверсии и теракты не только его возраст но и в ближайшее время может быть обжаловано в кассационной коллегии верховного суда россии отклонила жалобу избирательного объединения или блока или создать инициативную группу из 147 человек\n"
     ]
    }
   ],
   "source": [
    "print(generate_with_beam_search(matrix_news, id2word_news, bigram2id_news, start='другой'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 295,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "куда по его словам на сегодняшний день уже четверо членов ндр могут занять в нижней части бюллетеня расположено поле для гольфа оставив за собой право предъявить российскойстороне финансовые претензии по возможному урегулированию конфликта на лфз заключается в том числе и распространение вредоносных программ tribe flood network tfn и схожая с ней поставками товаров на которые содержится московское правительство работает не покладая рук но реально завершить свою работу после обеденного перерыва участниками митинга были избиты заведующая общим отделом аппарата правительства путин будет исполнять олег казаков ранее занимавший должность главного координатора ес по внешней политике нового правительства индии новая ядерная доктрина страны <end>\n"
     ]
    }
   ],
   "source": [
    "print(generate_with_beam_search(matrix_news, id2word_news, bigram2id_news, start='куда'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 296,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<start> <start> по в соответствии с указом президента посмертно награждены медалью анания ширакаци а депутаты ряда оппозиционных организаций вообще отказались от тактики позиционных боев и используют ее преимущественно для чатов игр переписки или поиска музыкальных файлов из интернета журналистами труда комсомольской правды новой газеты вячеслав измайлов яблоко автор и ведущей телепрограммы момент истины андрей караулов выдвинут избирателями глава общероссийского коммунистического общественного политического движения андрей брежнев самовыдвижение знаменитая лыжница елена вяльбе отизбирательного объединения кедр депутат госдумы второго созыва последовательно отстаивала курс на афганистан и направится в давос уже объявили 54 сообщает интерфакс телерадиокомпания петербург возобновляет свои передачи с 18.00 в пятницу в столице\n"
     ]
    }
   ],
   "source": [
    "print(generate_with_beam_search(matrix_news, id2word_news, bigram2id_news))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 297,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "голубое небо по в соответствии с указом президента посмертно награждены медалью анания ширакаци а депутаты ряда оппозиционных организаций вообще отказались от тактики позиционных боев и используют ее преимущественно для чатов игр переписки или поиска музыкальных файлов из интернета журналистами труда комсомольской правды новой газеты вячеслав измайлов яблоко автор и ведущей телепрограммы момент истины андрей караулов выдвинут избирателями глава общероссийского коммунистического общественного политического движения андрей брежнев самовыдвижение знаменитая лыжница елена вяльбе отизбирательного объединения кедр депутат госдумы второго созыва последовательно отстаивала курс на афганистан и направится в давос уже объявили 54 сообщает интерфакс телерадиокомпания петербург возобновляет свои передачи с 18.00 в пятницу в столице\n"
     ]
    }
   ],
   "source": [
    "print(generate_with_beam_search(matrix_news, id2word_news, bigram2id_news, start='голубое небо'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 299,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "совет федерации может в ближайшеевремя вообще утратить свои функции такой прогноз якушкина сильным сомнениям подверг эту информацию подтвердил министр информации югославии горан матич в интервью риа новости со ссылкой на источники в правоохранительных органах города интерфаксу сообщили в штабе объединенной группировки войск на северном кавказе генерал-полковник виктор казанцев геннадий трошев владимиршаманов и валентин корабельников а также по факсу по электронной почте или факсу и согласно идее автора должны вывешиваться в местах лишения свободы с отбыванием наказания в колонии усиленного режима приговорил суд армянского города гюмри двух российских транспортных самолетов ввс австралии для эвакуации персонала оон из города то братьям меньшим надо подыскать\n"
     ]
    }
   ],
   "source": [
    "print(generate_with_beam_search(matrix_news, id2word_news, bigram2id_news, start='совет федерации'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
