{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "1dba7c0d",
      "metadata": {
        "id": "1dba7c0d"
      },
      "source": [
        "# Домашнее задание № 10. Генерация текста"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "76f21d5e",
      "metadata": {
        "id": "76f21d5e"
      },
      "source": [
        "### Задание 1 (10 баллов).\n",
        "\n",
        "В семинаре мы работали с датасетами инструкций alpaca и dolly. Они англоязычные. В домашке вам нужно создать аналогичный датасет на русском языке и обучить аналогичную модель на этом датасете.\n",
        "В качестве итогового результата у вас должна получится модель, которая может связно отвечать на русскоязычные инструкции на русском языке. Приведите как минимум три разных примера. Правильность ответов не так важна, так как вы скорее всего будете использовать небольшие модели, но текст должен быть не рандомным.\n",
        "\n",
        "Русскоязычный датасет инструкций должен быть больше 5 тысяч примеров. Он может быть основнован на alpaca/dolly (например, вы можете просто прогнать все через переводную модель, которая была на семинаре, или даже google translate). Или вы можете придумать способ создать аналогичный датасет каким-то другим способом (переделать открытые датасеты с помощью правил). Датасет может быть не уникальным, можно скооперироваться с одногруппниками и сделать один датасет на всех.\n",
        "\n",
        "Вы можете попробовать дообучать любую небольшую decoder-only модель. Скорее всего лучше всего будут работать модели, изначально обученные на русском языке (rugpt например). Но возможно даже модели вроде opt можно будет дообучить на русскоязычных инструкциях.\n",
        "\n",
        "Это задание гораздно менее определенное, по сравнению с предыдущими. Поэтому не стесняйтесь задавать дополнительные вопросы в чате или лично, если у вас возникнут трудности."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# !pip install torch"
      ],
      "metadata": {
        "id": "fPpWT0Y5AZnz"
      },
      "id": "fPpWT0Y5AZnz",
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# !pip install accelerate==0.21.0"
      ],
      "metadata": {
        "id": "Xt2wR_y8BYTR"
      },
      "id": "Xt2wR_y8BYTR",
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install datasets zstandard jsonlines transformers==4.30"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "weOdg9zGOSO9",
        "outputId": "e26f1507-91d5-46f5-f72f-e12bb9222e53"
      },
      "id": "weOdg9zGOSO9",
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: datasets in /usr/local/lib/python3.10/dist-packages (2.19.0)\n",
            "Requirement already satisfied: zstandard in /usr/local/lib/python3.10/dist-packages (0.22.0)\n",
            "Requirement already satisfied: jsonlines in /usr/local/lib/python3.10/dist-packages (4.0.0)\n",
            "Requirement already satisfied: transformers==4.30 in /usr/local/lib/python3.10/dist-packages (4.30.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers==4.30) (3.13.4)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.14.1 in /usr/local/lib/python3.10/dist-packages (from transformers==4.30) (0.22.2)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers==4.30) (1.25.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers==4.30) (24.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers==4.30) (6.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers==4.30) (2023.12.25)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers==4.30) (2.31.0)\n",
            "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /usr/local/lib/python3.10/dist-packages (from transformers==4.30) (0.13.3)\n",
            "Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from transformers==4.30) (0.4.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers==4.30) (4.66.2)\n",
            "Requirement already satisfied: pyarrow>=12.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (14.0.2)\n",
            "Requirement already satisfied: pyarrow-hotfix in /usr/local/lib/python3.10/dist-packages (from datasets) (0.6)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.3.8)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets) (2.0.3)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from datasets) (3.4.1)\n",
            "Requirement already satisfied: multiprocess in /usr/local/lib/python3.10/dist-packages (from datasets) (0.70.16)\n",
            "Requirement already satisfied: fsspec[http]<=2024.3.1,>=2023.1.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (2023.6.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets) (3.9.5)\n",
            "Requirement already satisfied: attrs>=19.2.0 in /usr/local/lib/python3.10/dist-packages (from jsonlines) (23.2.0)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.3.1)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (6.0.5)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.9.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (4.0.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.14.1->transformers==4.30) (4.11.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==4.30) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==4.30) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==4.30) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==4.30) (2024.2.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2023.4)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2024.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "527aa69a",
      "metadata": {
        "id": "527aa69a"
      },
      "outputs": [],
      "source": [
        "import copy\n",
        "import logging\n",
        "from dataclasses import dataclass, field\n",
        "from typing import Optional, Dict, Sequence\n",
        "import json\n",
        "import torch\n",
        "import transformers\n",
        "from torch.utils.data import Dataset\n",
        "from transformers import Trainer"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "dataset = load_dataset(\"IlyaGusev/ru_turbo_alpaca\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rrmuRco5OJVi",
        "outputId": "d0a2e68c-86b0-45f4-8bba-0d25e818fb47"
      },
      "id": "rrmuRco5OJVi",
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_token.py:89: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/datasets/load.py:1486: FutureWarning: The repository for IlyaGusev/ru_turbo_alpaca contains custom code which must be executed to correctly load the dataset. You can inspect the repository content at https://hf.co/datasets/IlyaGusev/ru_turbo_alpaca\n",
            "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
            "Passing `trust_remote_code=True` will be mandatory to load this dataset from the next major release of `datasets`.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dataset"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jgG0xem9Wm1S",
        "outputId": "81cb63ca-03e7-4cf8-bb03-259802156247"
      },
      "id": "jgG0xem9Wm1S",
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "DatasetDict({\n",
              "    train: Dataset({\n",
              "        features: ['instruction', 'input', 'output', 'alternative_output', 'label', 'all_labels', 'agreement', 'overlap'],\n",
              "        num_rows: 29822\n",
              "    })\n",
              "})"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(0, 9):\n",
        "  print(dataset['train'][i]['instruction'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YfwrX6RDZkTI",
        "outputId": "1b83765e-b485-49ed-9bfd-3629f689500c"
      },
      "id": "YfwrX6RDZkTI",
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Опишите, как сделать горшок из глины.\n",
            "Опиши процесс изготовления торта. Укажи ингредиенты, необходимые для этого.\n",
            "Напиши аналог данной пословицы на современный лад: \"Не говори гоп, пока не перепрыгнешь\".\n",
            "Представьте, что вы занимаетесь дизайном главной страницы для нового сайта. Опишите, что вы хотели бы видеть на этой странице.\n",
            "Напиши рассказ о своем детстве и самом смешном инциденте, который произошел с тобой.\n",
            "Напишите краткую историю о двух лучших друзьях.\n",
            "Придумай название нового магазина.\n",
            "Напишите рекламное объявление о продукте или услуге, используя привлекательный язык и убедительные доводы.\n",
            "Предложите тему для диссертации в области обработки естественного языка.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import json"
      ],
      "metadata": {
        "id": "IEGipcw4cxnP"
      },
      "id": "IEGipcw4cxnP",
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "subset_list = []\n",
        "\n",
        "for i in range(0, 9999):\n",
        "    subset_dict = {\n",
        "        'instruction': dataset['train'][i]['instruction'],\n",
        "        'input': dataset['train'][i]['input'],\n",
        "        'output': dataset['train'][i]['output']\n",
        "    }\n",
        "    subset_list.append(subset_dict)\n",
        "\n",
        "with open(\"ru_alpaca.json\", \"w\") as json_file:\n",
        "    json.dump(subset_list, json_file, indent=2)"
      ],
      "metadata": {
        "id": "SMgvSkB_XCzm"
      },
      "id": "SMgvSkB_XCzm",
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# data_alpaca = json.load(open('ru_alpaca.json'))"
      ],
      "metadata": {
        "id": "ZLVaZfKIaYzy"
      },
      "id": "ZLVaZfKIaYzy",
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# del data_alpaca"
      ],
      "metadata": {
        "id": "GWqv4kJDcl6M"
      },
      "id": "GWqv4kJDcl6M",
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# len(data_alpaca)"
      ],
      "metadata": {
        "id": "V5wl70t9-GYc"
      },
      "id": "V5wl70t9-GYc",
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# data_alpaca[:2]"
      ],
      "metadata": {
        "id": "YIcCzZokaqaO"
      },
      "id": "YIcCzZokaqaO",
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "IGNORE_INDEX = -100\n",
        "DEFAULT_PAD_TOKEN = \"[PAD]\"\n",
        "DEFAULT_EOS_TOKEN = \"</s>\"\n",
        "DEFAULT_BOS_TOKEN = \"</s>\"\n",
        "DEFAULT_UNK_TOKEN = \"</s>\"\n",
        "PROMPT_DICT = {\n",
        "    \"prompt_input\": (\n",
        "        \"Ниже будут инструкции для решения задачи, а также текст, содержащий больше контекста. \"\n",
        "        \"Напиши отвечающий запросу ответ.\\n\\n\"\n",
        "        \"### Инструкции:\\n{instruction}\\n\\n### Контекст:\\n{input}\\n\\n### Ответ:\"\n",
        "    ),\n",
        "    \"prompt_no_input\": (\n",
        "        \"Ниже будут инструкции для решения задачи. \"\n",
        "        \"Напиши отвечающий запросу ответ.\\n\\n\"\n",
        "        \"### Инструкции:\\n{instruction}\\n\\n### Ответ:\"\n",
        "    ),\n",
        "}"
      ],
      "metadata": {
        "id": "HWe4JJ2TUBsq"
      },
      "id": "HWe4JJ2TUBsq",
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def _tokenize_fn(strings: Sequence[str], tokenizer: transformers.PreTrainedTokenizer) -> Dict:\n",
        "    \"\"\"Tokenize a list of strings.\"\"\"\n",
        "    tokenized_list = [\n",
        "        tokenizer(\n",
        "            text,\n",
        "            return_tensors=\"pt\",\n",
        "            max_length=tokenizer.model_max_length,\n",
        "            truncation=True,\n",
        "        )\n",
        "        for text in strings\n",
        "    ]\n",
        "    input_ids = labels = [tokenized.input_ids[0] for tokenized in tokenized_list]\n",
        "    input_ids_lens = labels_lens = [\n",
        "        tokenized.input_ids.ne(tokenizer.pad_token_id).sum().item() for tokenized in tokenized_list\n",
        "    ]\n",
        "    return dict(\n",
        "        input_ids=input_ids,\n",
        "        labels=labels,\n",
        "        input_ids_lens=input_ids_lens,\n",
        "        labels_lens=labels_lens,\n",
        "    )"
      ],
      "metadata": {
        "id": "HcUmjICVUEBP"
      },
      "id": "HcUmjICVUEBP",
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess(\n",
        "    sources: Sequence[str],\n",
        "    targets: Sequence[str],\n",
        "    tokenizer: transformers.PreTrainedTokenizer,\n",
        ") -> Dict:\n",
        "    \"\"\"Preprocess the data by tokenizing.\"\"\"\n",
        "    examples = [s + t for s, t in zip(sources, targets)]\n",
        "    examples_tokenized, sources_tokenized = [_tokenize_fn(strings, tokenizer) for strings in (examples, sources)]\n",
        "    input_ids = examples_tokenized[\"input_ids\"]\n",
        "    labels = copy.deepcopy(input_ids)\n",
        "    for label, source_len in zip(labels, sources_tokenized[\"input_ids_lens\"]):\n",
        "        label[:source_len] = IGNORE_INDEX\n",
        "    return dict(input_ids=input_ids, labels=labels)"
      ],
      "metadata": {
        "id": "R0vu9EpaUIvQ"
      },
      "id": "R0vu9EpaUIvQ",
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class SupervisedDataset(Dataset):\n",
        "    \"\"\"Dataset for supervised fine-tuning.\"\"\"\n",
        "\n",
        "    def __init__(self, data_path: str, tokenizer: transformers.PreTrainedTokenizer):\n",
        "        super(SupervisedDataset, self).__init__()\n",
        "        logging.warning(\"Loading data...\")\n",
        "        list_data_dict = json.load(open(data_path))\n",
        "\n",
        "        logging.warning(\"Formatting inputs...\")\n",
        "        prompt_input, prompt_no_input = PROMPT_DICT[\"prompt_input\"], PROMPT_DICT[\"prompt_no_input\"]\n",
        "        sources = [\n",
        "            prompt_input.format_map(example) if example.get(\"input\", \"\") != \"\" else prompt_no_input.format_map(example)\n",
        "            for example in list_data_dict\n",
        "        ]\n",
        "        targets = [f\"{example['output']}{tokenizer.eos_token}\" for example in list_data_dict]\n",
        "\n",
        "        logging.warning(\"Tokenizing inputs... This may take some time...\")\n",
        "        data_dict = preprocess(sources, targets, tokenizer)\n",
        "\n",
        "        self.input_ids = data_dict[\"input_ids\"]\n",
        "        self.labels = data_dict[\"labels\"]\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.input_ids)\n",
        "\n",
        "    def __getitem__(self, i) -> Dict[str, torch.Tensor]:\n",
        "        return dict(input_ids=self.input_ids[i], labels=self.labels[i])\n",
        "\n",
        "\n",
        "@dataclass\n",
        "class DataCollatorForSupervisedDataset(object):\n",
        "    \"\"\"Collate examples for supervised fine-tuning.\"\"\"\n",
        "\n",
        "    tokenizer: transformers.PreTrainedTokenizer\n",
        "\n",
        "    def __call__(self, instances: Sequence[Dict]) -> Dict[str, torch.Tensor]:\n",
        "        input_ids, labels = tuple([instance[key] for instance in instances] for key in (\"input_ids\", \"labels\"))\n",
        "        input_ids = torch.nn.utils.rnn.pad_sequence(\n",
        "            input_ids, batch_first=True, padding_value=self.tokenizer.pad_token_id\n",
        "        )\n",
        "        labels = torch.nn.utils.rnn.pad_sequence(labels, batch_first=True, padding_value=IGNORE_INDEX)\n",
        "        return dict(\n",
        "            input_ids=input_ids,\n",
        "            labels=labels,\n",
        "            attention_mask=input_ids.ne(self.tokenizer.pad_token_id),\n",
        "        )"
      ],
      "metadata": {
        "id": "pKUoeFT2ULWx"
      },
      "id": "pKUoeFT2ULWx",
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"ai-forever/rugpt3small_based_on_gpt2\")\n",
        "model = AutoModelForCausalLM.from_pretrained(\"ai-forever/rugpt3small_based_on_gpt2\")"
      ],
      "metadata": {
        "id": "p_Cl9HsOUQke"
      },
      "id": "p_Cl9HsOUQke",
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset = SupervisedDataset(tokenizer=tokenizer, data_path=\"ru_alpaca.json\")\n",
        "data_collator = DataCollatorForSupervisedDataset(tokenizer=tokenizer)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JCcjeDdNUmRM",
        "outputId": "479a32ef-ca3b-4992-f49c-3953e71f049a"
      },
      "id": "JCcjeDdNUmRM",
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:root:Loading data...\n",
            "WARNING:root:Formatting inputs...\n",
            "WARNING:root:Tokenizing inputs... This may take some time...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_args = transformers.TrainingArguments(learning_rate=1e-4,\n",
        "                 num_train_epochs=2,\n",
        "                 per_device_train_batch_size=2,\n",
        "                 gradient_accumulation_steps=1,\n",
        "                 evaluation_strategy='no',\n",
        "                 weight_decay=0.1,\n",
        "                 warmup_ratio=0.03,\n",
        "                 lr_scheduler_type=\"cosine\",\n",
        "                 save_strategy='no',\n",
        "                 logging_steps=1000,\n",
        "                 output_dir=\"rugpt_instruct_ft\")"
      ],
      "metadata": {
        "id": "9gGF6bYA-WjK"
      },
      "id": "9gGF6bYA-WjK",
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainer = Trainer(model=model,\n",
        "                 tokenizer=tokenizer,\n",
        "                 args=train_args,\n",
        "                 train_dataset=train_dataset,\n",
        "                 eval_dataset=None,\n",
        "                 data_collator=data_collator)"
      ],
      "metadata": {
        "id": "PM4JCUCw-itk"
      },
      "id": "PM4JCUCw-itk",
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainer.train()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 478
        },
        "id": "9xc1mMrt-mIo",
        "outputId": "43e2c71d-ec00-47b5-cbdf-1dc20473bf4d"
      },
      "id": "9xc1mMrt-mIo",
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='10000' max='10000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [10000/10000 21:34, Epoch 2/2]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1000</td>\n",
              "      <td>2.627500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2000</td>\n",
              "      <td>2.589400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3000</td>\n",
              "      <td>2.527300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4000</td>\n",
              "      <td>2.459800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5000</td>\n",
              "      <td>2.392100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>6000</td>\n",
              "      <td>1.608800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>7000</td>\n",
              "      <td>1.572200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>8000</td>\n",
              "      <td>1.544000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>9000</td>\n",
              "      <td>1.531100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>10000</td>\n",
              "      <td>1.511900</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TrainOutput(global_step=10000, training_loss=2.0364072265625, metrics={'train_runtime': 1294.862, 'train_samples_per_second': 15.444, 'train_steps_per_second': 7.723, 'total_flos': 1402499271168000.0, 'train_loss': 2.0364072265625, 'epoch': 2.0})"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "trainer.save_model('rugtp_ft')"
      ],
      "metadata": {
        "id": "uaYB6oNvJd7R"
      },
      "id": "uaYB6oNvJd7R",
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def predict_for_instruction(instruction, text, model, tokenizer):\n",
        "    text = text.replace('\\n', ' ')\n",
        "    prompt = (\"Ниже будут инструкции для решения задачи, а также текст, содержащий больше контекста. \"\n",
        "              \"Напиши отвечающий запросу ответ.\\n\\n\"\n",
        "              f\"### Инструкции:\\n{instruction}\\n\\n### Контекст:\\n{text}\\n\\n### Ответ:\")\n",
        "\n",
        "    inputs = tokenizer([prompt], return_tensors=\"pt\", padding=True)\n",
        "\n",
        "    inputs = {key: tensor.to(model.device) for key, tensor in inputs.items()}\n",
        "\n",
        "    with torch.no_grad():\n",
        "        output_sequences = model.generate(\n",
        "            max_length=2000,\n",
        "            early_stopping=True,\n",
        "            num_return_sequences=1,\n",
        "            **inputs\n",
        "        )\n",
        "\n",
        "    output = tokenizer.batch_decode(output_sequences[:, len(tokenizer.encode(prompt)):], skip_special_tokens=True)\n",
        "    return output[0]\n"
      ],
      "metadata": {
        "id": "yMccWE5sHXjN"
      },
      "id": "yMccWE5sHXjN",
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text = \"\"\"\n",
        "\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "SXIVxadRFAMU"
      },
      "id": "SXIVxadRFAMU",
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "instruction = \"Напиши краткую рекомендацию о том, как приготовить пиццу.\"\n",
        "predict_for_instruction(instruction, text, model, tokenizer)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "id": "nz3Ko56sFRgd",
        "outputId": "0c9db353-b8c4-4911-9703-61212688cd6a"
      },
      "id": "nz3Ko56sFRgd",
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Для приготовления пиццы нужно взять тесто, раскатать его на плоской поверхности и нанести на нее соус. Выложить тесто на противень и запекать в духовке при температуре 200 градусов в течение 10-15 минут.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "text = \"\"\"\n",
        "Штирлиц играл в карты и проигрался. Но Штирлиц умел делать хорошую мину при плохой игре. Когда Штирлиц покинул компанию, мина сработала.\n",
        "\n",
        "Идет Штирлиц ночью по городу, навстречу мужик бородатый и в чалме.\n",
        "— Ах, будь он не Ладен - подумал Штирлиц.\n",
        "\n",
        "Штирлиц и Мюллер ездили по очереди на танке. Очередь редела, но не расходилась...\n",
        "\n",
        "Штирлиц стрелял вслепую. Слепая испугалась и побежала скачками, но качки быстро отстали.\n",
        "\n",
        "Подвыпившие Штирлиц и Мюллер вышли из бара.\n",
        "— Давайте снимем девочек, — предложил Штирлиц.\n",
        "— У вас очень доброе сердце, — ответил Мюллер. — Но пусть все-таки повисят до утра.\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "Ca_pNkzJJ4uz"
      },
      "id": "Ca_pNkzJJ4uz",
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "instruction = \"Выбери самый смешной анекдот.\"\n",
        "predict_for_instruction(instruction, text, model, tokenizer)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "id": "DhrPrUWpIXN8",
        "outputId": "b7755a74-e661-406f-d728-f517481f355f"
      },
      "id": "DhrPrUWpIXN8",
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Штирлиц был очень рад, что смог выиграть игру. Он был рад, что смог выиграть эту партию. Но когда он вернулся домой, он был очень расстроен тем, что проиграл. Он был рад, что смог выиграть эту партию.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "text = \"\"\"\n",
        "\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "Acb87NhbJyN_"
      },
      "id": "Acb87NhbJyN_",
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "instruction = \"Почему Земля круглая?\"\n",
        "predict_for_instruction(instruction, text, model, tokenizer)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "VKzqNaqvJmYE",
        "outputId": "f18e747a-cad2-49fe-87ef-980c87833a93"
      },
      "id": "VKzqNaqvJmYE",
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Земля круглая - это эллиптическая форма, в которой центр тяжести находится между Землей и Солнцем.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "instruction = \"Придумай анекдот.\"\n",
        "predict_for_instruction(instruction, text, model, tokenizer)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "id": "FCyffx4pKrig",
        "outputId": "77a3cfc9-cf2e-4da4-b818-8128133acf6f"
      },
      "id": "FCyffx4pKrig",
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Встретились два студента в университете. Один из них спросил: \"Как тебе наша общая задача?\" \\nВторой студент ответил: \"Да, это наша общая задача.\"'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 36
        }
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.16"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}