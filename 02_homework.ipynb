{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "43fc8290",
   "metadata": {},
   "source": [
    "## Задание 1."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b06a5ace",
   "metadata": {},
   "source": [
    "Посчитайте частоты для 5-грамм в корпусе lenta.txt. двумя способами:  \n",
    "1) lenta.txt -> sent_tokenize (russian) -> word_tokenize -> ngrammer  \n",
    "2) lenta.txt -> word_tokene(preserve_line=True) - ngrammer  \n",
    "    \n",
    "Проанализируйте топ-20 самых частотных нграмм и проверьте есть ли различия? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "957f5656",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('риа новости со ссылкой на', 400),\n",
       " ('сообщает риа новости со ссылкой', 320),\n",
       " ('как сообщили риа новости в', 196),\n",
       " ('как сообщает риа новости со', 149),\n",
       " ('сообщает интерфакс со ссылкой на', 142),\n",
       " ('сообщает итар-тасс со ссылкой на', 118),\n",
       " ('об этом риа новости сообщили', 113),\n",
       " ('об этом сообщает риа новости', 104),\n",
       " ('этом риа новости сообщили в', 99),\n",
       " ('со ссылкой на источники в', 93),\n",
       " ('сообщили риа новости в пресс-службе', 88),\n",
       " ('группировки войск на северном кавказе', 84),\n",
       " ('как сообщает интерфакс со ссылкой', 83),\n",
       " ('объединенной группировки войск на северном', 83),\n",
       " ('новости со ссылкой на пресс-службу', 76),\n",
       " ('эхо москвы со ссылкой на', 76),\n",
       " ('этом сообщает риа новости со', 75),\n",
       " ('в связи с тем что', 70),\n",
       " ('по борьбе с организованной преступностью', 66),\n",
       " ('как сообщает итар-тасс со ссылкой', 58)]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk import sent_tokenize\n",
    "from nltk.tokenize import word_tokenize\n",
    "from collections import Counter\n",
    "corpus = open('data/lenta.txt', encoding=\"utf-8\").read()\n",
    "sentences = sent_tokenize(corpus, language='russian')\n",
    "tokenized_sentences = [word_tokenize(sentence) for sentence in sentences]\n",
    "\n",
    "import re\n",
    "tokenized_sentences = [[token.lower() for token in sentence if not re.match('\\W+', token)] \n",
    "                       for sentence in tokenized_sentences]\n",
    "\n",
    "def ngrammer(tokens, n=5):\n",
    "    ngrams = []\n",
    "    for i in range(len(tokens)-n+1):\n",
    "        ngrams.append(' '.join(tokens[i:i+n]))\n",
    "    return ngrams\n",
    "\n",
    "pentagram_counts = Counter()\n",
    "for sentence in tokenized_sentences:\n",
    "    pentagram_counts.update(ngrammer([token for token in sentence]))\n",
    "\n",
    "pentagram_counts.most_common(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8e9dd7c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "corpus = open('data/lenta.txt', encoding=\"utf-8\").read()\n",
    "tokenized_sentences = [word_tokenize(corpus, preserve_line=True)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "20660e4e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('риа новости со ссылкой на', 400),\n",
       " ('сообщает риа новости со ссылкой', 320),\n",
       " ('как сообщили риа новости в', 196),\n",
       " ('как сообщает риа новости со', 149),\n",
       " ('сообщает интерфакс со ссылкой на', 142),\n",
       " ('сообщает итар-тасс со ссылкой на', 118),\n",
       " ('об этом риа новости сообщили', 113),\n",
       " ('об этом сообщает риа новости', 104),\n",
       " ('этом риа новости сообщили в', 99),\n",
       " ('со ссылкой на источники в', 93),\n",
       " ('сообщили риа новости в пресс-службе', 88),\n",
       " ('как сообщает интерфакс со ссылкой', 83),\n",
       " ('объединенной группировки войск на северном', 83),\n",
       " ('эхо москвы со ссылкой на', 77),\n",
       " ('новости со ссылкой на пресс-службу', 76),\n",
       " ('этом сообщает риа новости со', 75),\n",
       " ('в связи с тем что', 70),\n",
       " ('как сообщает итар-тасс со ссылкой', 58),\n",
       " ('группировки войск на северном кавказе', 57),\n",
       " ('по борьбе с организованной преступностью', 55)]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "tokenized_sentences = [[token.lower() for token in sentence if not re.match('\\W+', token)] \n",
    "                       for sentence in tokenized_sentences]\n",
    "\n",
    "def ngrammer(tokens, n=5):\n",
    "    ngrams = []\n",
    "    for i in range(len(tokens)-n+1):\n",
    "        ngrams.append(' '.join(tokens[i:i+n]))\n",
    "    return ngrams\n",
    "\n",
    "from collections import Counter\n",
    "pentagram_counts = Counter()\n",
    "for sentence in tokenized_sentences:\n",
    "    pentagram_counts.update(ngrammer([token for token in sentence]))\n",
    "\n",
    "pentagram_counts.most_common(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c012f204",
   "metadata": {},
   "source": [
    "Есть различия в частотности нескольких отдельных 5-грамм."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5781f34",
   "metadata": {},
   "source": [
    "## Задание 2."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4292716e",
   "metadata": {},
   "source": [
    "Найдите какую-то инетересную (по вашему мнению) закономерность на https://books.google.com/ngrams/ для русского языка (с 1990 по 2019)\n",
    "\n",
    "Вставьте сюда скриншот\n",
    "\n",
    "(В конце 2015 года произошло объединение нескольких районов Московской области)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e3bf76d",
   "metadata": {},
   "source": [
    "![](data/Ngram.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a0a89ec",
   "metadata": {},
   "source": [
    "## Заданиe 3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b40c35e9",
   "metadata": {},
   "source": [
    "Когда мы разбирали PMI мы использовали такую функцию:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "221f1bc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scorer_simple(word_count_a, word_count_b, bigram_count, *args):\n",
    "    try:\n",
    "        score = bigram_count/((word_count_a+word_count_b))\n",
    "    except ZeroDivisionError:\n",
    "        return 0\n",
    "    return score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53fd2def",
   "metadata": {},
   "source": [
    "Но если вы посмотрите на определение в википедии, то увидите, что формула немного другая ![](https://wikimedia.org/api/rest_v1/media/math/render/svg/094243d23c19d2d032f6bb26c4dc4f47d98d32f8)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1905862",
   "metadata": {},
   "source": [
    "Перепишите функцию, чтобы она точно соответствовала этому определению. Расчитайте PMI для всех биграммов также как мы делали в семинаре с помощью функции score_bigrams используя изначальный scorer и обновленный. Посмотрите есть ли разница в топ-10 биграммов. Подумайте почему результаты совпадают/отличаются?\n",
    "\n",
    "*Подсказка: для вероятностей можно поделить на количество слов в корпусе"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "1431f618",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1493813"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus_length = 0\n",
    "token_counts = Counter()\n",
    "for sentence in tokenized_sentences:\n",
    "    token_counts.update(sentence)\n",
    "    corpus_length += len(sentence)\n",
    "\n",
    "import numpy as np\n",
    "def scorer_updated(word_count_a, word_count_b, bigram_count, *args):\n",
    "    try:\n",
    "        score = np.log2(bigram_count/corpus_length)/((word_count_a/corpus_length)*(word_count_b/corpus_length))\n",
    "    except ZeroDivisionError:\n",
    "        return 0\n",
    "    return score\n",
    "\n",
    "#corpus_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "e340497c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Dmitry\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords')\n",
    "\n",
    "russian_stopwords = set(stopwords.words('russian'))\n",
    "\n",
    "def scorer(word_count_a, word_count_b, bigram_count, min_count=0):\n",
    "    try:\n",
    "        score = ((bigram_count - min_count) / ((word_count_a + word_count_b)))\n",
    "    except ZeroDivisionError:\n",
    "        return 0\n",
    "    \n",
    "    return score\n",
    "\n",
    "def ngrammer(tokens, n=2, stops=set()):\n",
    "    ngrams = []\n",
    "    tokens = [token for token in tokens if token not in stops]\n",
    "    for i in range(0,len(tokens)-n+1):\n",
    "        ngrams.append(' '.join(tokens[i:i+n]))\n",
    "    return ngrams\n",
    "\n",
    "def collect_stats(corpus, stops):\n",
    "    ## соберем статистики для отдельных слов\n",
    "    ## и биграммов\n",
    "    \n",
    "    unigrams = Counter()\n",
    "    bigrams = Counter()\n",
    "    \n",
    "    for sent in corpus:\n",
    "        unigrams.update(sent)\n",
    "        bigrams.update(ngrammer(sent, 2, stops))\n",
    "    \n",
    "    return unigrams, bigrams\n",
    "\n",
    "\n",
    "def score_bigrams(unigrams, bigrams, scorer, threshold=-100000, min_count=0):\n",
    "    ## посчитаем метрику для каждого нграмма\n",
    "    bigram2score = Counter()\n",
    "    \n",
    "    for bigram in bigrams:\n",
    "        word_a, word_b = bigram.split()\n",
    "        score = scorer(unigrams[word_a], unigrams[word_b], bigrams[bigram], min_count)\n",
    "        \n",
    "        ## если метрика выше порога, добавляем в словарик\n",
    "        if score > threshold:\n",
    "            bigram2score[bigram] = score\n",
    "    \n",
    "    return bigram2score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "e478fb1e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('риа новости', 0.49026798307475317),\n",
       " ('сих пор', 0.3831417624521073),\n",
       " ('new york', 0.38071065989847713),\n",
       " ('таким образом', 0.36951316839584997),\n",
       " ('северном кавказе', 0.3631156930126002),\n",
       " ('рао еэс', 0.3382663847780127),\n",
       " ('объединенной группировки', 0.2992036405005688),\n",
       " ('возбуждено уголовное', 0.2988505747126437),\n",
       " ('wall street', 0.29523809523809524),\n",
       " ('чрезвычайным ситуациям', 0.2878787878787879)]"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unigrams, bigrams = collect_stats(tokenized_sentences, russian_stopwords)\n",
    "bigram2score = score_bigrams(unigrams, bigrams, scorer, min_count=20)\n",
    "bigram2score.most_common(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "3f7bcd31",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('сопоцкина друскеник', 0.5),\n",
       " ('германцев. неприятель', 0.5),\n",
       " ('неприятель приблизившись', 0.5),\n",
       " ('саноку обстреливалась', 0.5),\n",
       " ('парки обоз', 0.5),\n",
       " ('м.ю. лермонтова', 0.5),\n",
       " ('австрийский аэроплан', 0.5),\n",
       " ('нестеров. показывался', 0.5),\n",
       " ('показывался аэроплан-птица', 0.5),\n",
       " ('das ist', 0.5)]"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unigrams, bigrams = collect_stats(tokenized_sentences, russian_stopwords)\n",
    "bigram2score = score_bigrams(unigrams, bigrams, scorer_simple)\n",
    "bigram2score.most_common(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "db3949c2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unigrams, bigrams = collect_stats(tokenized_sentences, russian_stopwords)\n",
    "bigram2score = score_bigrams(unigrams, bigrams, scorer_updated)\n",
    "bigram2score.most_common(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a6e1c99",
   "metadata": {},
   "source": [
    "## Задание 4*\n",
    "\n",
    "Обновите функцию получившуюся в предыдущем задании так, чтобы вместо произведения/деления вероятностей использовались сложение и вычитание логирифмов. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f55a362",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b22785f4",
   "metadata": {},
   "source": [
    "## Задание 5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1121e53",
   "metadata": {},
   "source": [
    "Исследуйте gensim.models.Phrases. Проверьте сколько дефолтных scoring функций есть в этом классе. Попробуйте все доступные по умолчанию scoring функции и попробуйте настраивать для них значение threshold и min_count. Попробуйте сделать так, чтобы собиралось как можно больше нграммов. Попробуйте строить последовательность gensim.models.Phrases, чтобы строить более длинные нграммы"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "716fba84",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "from nltk import sent_tokenize\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "corpus = open('data/lenta.txt', encoding=\"utf-8\").read()\n",
    "sentences = sent_tokenize(corpus, language='russian')\n",
    "tokenized_sentences = [word_tokenize(sentence) for sentence in sentences]\n",
    "\n",
    "import re\n",
    "tokenized_sentences = [[token.lower() for token in sentence if not re.match('\\W+', token)] \n",
    "                       for sentence in tokenized_sentences]\n",
    "\n",
    "def scorer_w2v(worda_count, wordb_count, bigram_count, corpus_word_count, len_vocab=0,  min_count=0):\n",
    "\n",
    "    try:\n",
    "        score = ((bigram_count - min_count) * corpus_word_count) / (worda_count * wordb_count)\n",
    "    except ZeroDivisionError:\n",
    "        return 0\n",
    "    \n",
    "    return score\n",
    "\n",
    "ph = gensim.models.Phrases(tokenized_sentences, \n",
    "                           min_count=1, \n",
    "                           threshold=1.,\n",
    "                           scoring=scorer_w2v)\n",
    "p = gensim.models.phrases.Phraser(ph)\n",
    "\n",
    "ph2 = gensim.models.Phrases(p[tokenized_sentences],  min_count=1, threshold=1., scoring=scorer_w2v)\n",
    "p2 = gensim.models.phrases.Phraser(ph2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "c219e4e1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['с_раннего',\n",
       " 'утра',\n",
       " '14_сентября',\n",
       " 'огонь',\n",
       " 'достиг',\n",
       " 'значительного',\n",
       " 'напряжения']"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_sentences[3]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "6ad39387",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['с_раннего',\n",
       " 'утра',\n",
       " '14_сентября',\n",
       " 'огонь',\n",
       " 'достиг',\n",
       " 'значительного',\n",
       " 'напряжения']"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p[tokenized_sentences[3]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "da42aee1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['с_раннего_утра',\n",
       " '14_сентября',\n",
       " 'огонь',\n",
       " 'достиг',\n",
       " 'значительного',\n",
       " 'напряжения']"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p2[p[tokenized_sentences[3]]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "b627a663",
   "metadata": {},
   "outputs": [],
   "source": [
    "ph_u = gensim.models.Phrases(tokenized_sentences, \n",
    "                           min_count=99, \n",
    "                           threshold=4.,\n",
    "                           scoring=scorer_w2v)\n",
    "p_u = gensim.models.phrases.Phraser(ph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "68085f5d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['русский_инвалид',\n",
       " '16_сентября',\n",
       " '1914',\n",
       " 'года.министерство',\n",
       " 'народного',\n",
       " 'просвещения',\n",
       " 'в_виду',\n",
       " 'происходящих',\n",
       " 'чрезвычайных',\n",
       " 'событий',\n",
       " 'признало',\n",
       " 'соответственным',\n",
       " 'в_день',\n",
       " 'годовщины_со',\n",
       " 'дня_рождения',\n",
       " 'м.ю',\n",
       " 'лермонтова',\n",
       " '2-го',\n",
       " 'октября',\n",
       " '1914_года',\n",
       " 'ограничиться',\n",
       " 'совершением',\n",
       " 'в_учебных',\n",
       " 'заведениях',\n",
       " 'панихиды_по',\n",
       " 'поэту',\n",
       " 'отложив',\n",
       " 'празднование',\n",
       " 'юбилея',\n",
       " 'до',\n",
       " 'более',\n",
       " 'благоприятного',\n",
       " 'времени']"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p[tokenized_sentences[10]]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
